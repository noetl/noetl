apiVersion: noetl.io/v1
kind: Playbook
# Iterator note: this playbook uses the new iterator task structure
#   - type: iterator
#   - collection: "{{ <expr> }}"
#   - element: <name>

metadata:
  name: http_duckdb_postgres
  path: examples/test/http_duckdb_postgres


workload:
  message: "HTTP -> DuckDB -> Postgres pipeline"
  cities:
    - name: "London"
      lat: 51.51
      lon: -0.13
    - name: "Paris"
      lat: 48.85
      lon: 2.35
    - name: "Berlin"
      lat: 52.52
      lon: 13.41
  base_url: "https://api.open-meteo.com/v1"
  gcs_bucket: "noetl-demo-19700101"   # set this to an actual bucket; requires HMAC key_id/secret_key

workflow:
  - step: start
    desc: "Start pipeline"
    next:
      - step: ensure_pg_table

  - step: ensure_pg_table
    desc: "Ensure raw HTTP results table exists in Postgres"
    type: postgres
    auth: pg_local
    command: |
      CREATE TABLE IF NOT EXISTS public.weather_http_raw (
        id TEXT PRIMARY KEY,
        execution_id TEXT,
        iter_index INTEGER,
        city TEXT,
        url TEXT,
        elapsed DOUBLE PRECISION,
        payload TEXT,
        created_at TIMESTAMPTZ DEFAULT now()
      );
    next:
      - step: http_loop
        input:
          cities: "{{ workload.cities }}"

  - step: http_loop
    desc: "Fetch hourly temperatures for each city and save raw rows"
    type: iterator
    collection: "{{ cities }}"
    element: city
    mode: async
    task:
      type: http
      endpoint: "{{ workload.base_url }}/forecast"
      headers:
        User-Agent: "NoETL City HTTP -> PG Demo/1.0"
      params:
        latitude: "{{ city.lat }}"
        longitude: "{{ city.lon }}"
        hourly: "temperature_2m"
        forecast_days: 1
      save:
        storage:
          kind: postgres
          auth: pg_local
        table: public.weather_http_raw
        mode: upsert
        key: id
        data:
          id: "{{ execution_id }}:{{ city.name }}:{{ _loop.current_index }}"
          execution_id: "{{ execution_id }}"
          iter_index: "{{ _loop.current_index }}"
          city: "{{ city.name }}"
          url: "{{ data.url }}"
          elapsed: "{{ data.elapsed | default(0) }}"
          payload: "{{ data | tojson }}"
    next:
      - step: aggregate_with_duckdb

  # Credentials are now referenced by alias directly in the DuckDB step below,
  # no need to fetch decrypted payloads via HTTP steps.
  # Proceed directly to aggregation.
  - step: aggregate_with_duckdb
    desc: "Read Postgres rows in DuckDB, aggregate, write to GCS using credential aliases"
    type: duckdb
    with:
      # Compute destination base URI from env flag
      output_uri_base: "{{ ('gs://' + workload.gcs_bucket) if (env.NOETL_ENABLE_GCS | default('false')) in ['true','1','yes','on','True'] else 'data' }}"
    credentials:
      # ATTACH alias: pick the same name as you'll use in SQL
      pg_db:
        key: pg_local
      # SECRET alias: pick the same name you'll use in CREATE SECRET
      gcs_secret:
        key: gcs_hmac_local
    commands: |
      -- Load needed extensions
      INSTALL postgres; LOAD postgres;
      INSTALL httpfs;  LOAD httpfs;

      -- Preferred: attach using a named DuckDB SECRET created from credentials alias
      -- The plugin now auto-creates a POSTGRES SECRET named 'pg_db' from credential 'pg_local'.
      ATTACH '' AS pg_db (TYPE postgres, SECRET pg_db);

      # GCS/S3 secrets are auto-created from credentials mapping when present; no SQL needed here.

      -- Flattened view from Postgres-attached table
      CREATE OR REPLACE TABLE weather_flat AS
      SELECT id, city, url AS source_url, elapsed AS elapsed_sec, payload
      FROM pg_db.public.weather_http_raw
      WHERE execution_id = '{{ execution_id }}';

      -- Aggregate by city (simple count)
      CREATE OR REPLACE TABLE weather_agg AS
      SELECT city, COUNT(*) AS rows_per_city
      FROM weather_flat
      GROUP BY city;

      -- Write results
      -- Destination path is computed via with.output_uri_base
      COPY weather_flat TO '{{ output_uri_base }}/flat_{{ execution_id }}.parquet' (FORMAT PARQUET);
      COPY weather_agg  TO '{{ output_uri_base }}/agg_{{ execution_id }}.parquet'  (FORMAT PARQUET);
    next:
      - step: ensure_metrics_table

  - step: ensure_metrics_table
    desc: "Ensure metrics table exists in Postgres"
    type: postgres
    auth: pg_local
    command: |
      CREATE TABLE IF NOT EXISTS public.weather_pipeline_metrics (
        execution_id TEXT PRIMARY KEY,
        pg_rows_saved INTEGER,
        gcs_flat_uri TEXT,
        gcs_agg_uri  TEXT,
        created_at   TIMESTAMPTZ DEFAULT now()
      );
    next:
      - step: end

  - step: end
    desc: "Finish"
    type: save
    save:
      storage:
        kind: postgres
        auth: pg_local
      statement: |
        INSERT INTO public.weather_pipeline_metrics (execution_id, pg_rows_saved, gcs_flat_uri, gcs_agg_uri)
        VALUES (
          '{{ execution_id }}',
          (SELECT COUNT(*) FROM public.weather_http_raw WHERE execution_id = '{{ execution_id }}'),
          'gs://{{ workload.gcs_bucket }}/weather/flat_{{ execution_id }}.parquet',
          'gs://{{ workload.gcs_bucket }}/weather/agg_{{ execution_id }}.parquet'
        )
        ON CONFLICT (execution_id) DO UPDATE SET
          pg_rows_saved = EXCLUDED.pg_rows_saved,
          gcs_flat_uri = EXCLUDED.gcs_flat_uri,
          gcs_agg_uri = EXCLUDED.gcs_agg_uri
