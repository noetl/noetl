apiVersion: noetl.io/v1
kind: Playbook
name: gs_duckdb_postgres_example
path: examples/gs_duckdb_postgres_example
workload:
  jobId: '{{ job.uuid }}'
  execution_id: '{{ job.uuid }}'
  gs_project_id: noetl-demo-19700101
  gs_bucket: noetl-demo-19700101
  gcs_key_id_secret: '{{ env.GCS_KEY_ID_SECRET | default(''s3_access_key_id'') }}'
  gcs_secret_key_secret: '{{ env.GCS_SECRET_KEY_SECRET | default(''s3_secret_access_key'')
    }}'
  pg_password_secret: postgres-dev-password
  source_csv_path: data/test/test_data.csv
  local_csv_path: /tmp/test_data.csv
  local_parquet_path: /tmp/test_data.parquet
  gs_csv_path: uploads/test_data.csv
  gs_parquet_path: uploads/test_data.parquet
  pg_host: '{{ env.POSTGRES_HOST | default(''localhost'') }}'
  pg_port: '{{ env.POSTGRES_PORT | default(''5432'') }}'
  pg_user: '{{ env.POSTGRES_USER | default(''demo'') }}'
  pg_password: '{{ env.POSTGRES_PASSWORD | default(''demo'') }}'
  pg_db: '{{ env.POSTGRES_DB | default(''demo_noetl'') }}'
  source_table_name: test_data_table
  table_name: test_data
workflow:
- step: start
  desc: Start GS DuckDB Postgres Example Workflow
  next:
  - step: setup_duckdb_infrastructure
- step: setup_duckdb_infrastructure
  desc: Create DuckDB directory structure and database file
  type: workbook
  task: setup_duckdb_task
  next:
  - step: get_gcs_key_id
- step: get_gcs_key_id
  desc: Retrieve GCS HMAC Key ID from Google Secret Manager
  type: secrets
  provider: google
  project_id: noetl-demo-19700101
  secret_name: s3_access_key_id
  next:
  - step: get_gcs_secret_key
- step: get_gcs_secret_key
  desc: Retrieve GCS HMAC Secret Key from Google Secret Manager
  type: secrets
  provider: google
  project_id: noetl-demo-19700101
  secret_name: s3_secret_access_key
  next:
  - step: get_pg_password
- step: get_pg_password
  desc: Retrieve Postgres Password from Google Secret Manager
  type: secrets
  provider: google
  project_id: noetl-demo-19700101
  secret_name: postgres-dev-password
  next:
  - step: create_gcs_secret
- step: create_gcs_secret
  desc: Create Duckdb secret for GCS authentication
  type: workbook
  task: create_gcs_secret_task
  input:
    key_id: '{{ get_gcs_key_id.secret_value }}'
    secret_key: '{{ get_gcs_secret_key.secret_value }}'
  next:
  - step: create_pg_secret
- step: create_pg_secret
  desc: Create Duckdb secret for Postgres authentication
  type: workbook
  task: create_pg_secret_task
  input:
    pg_user: '{{ workload.pg_user }}'
    pg_password: '{{ get_pg_password.secret_value }}'
    execution_id: '{{ workload.execution_id }}'
  next:
  - step: test_gcs_credentials
- step: test_gcs_credentials
  desc: Test GCS credentials by attempting to list bucket or create a test file
  type: workbook
  task: test_gcs_credentials
  input:
    key_id: '{{ get_gcs_key_id.secret_value }}'
    secret_key: '{{ get_gcs_secret_key.secret_value }}'
  next:
  - step: read_from_postgres
- step: read_from_postgres
  desc: Read data from Postgres test_data_table and store on local filesystem
  type: workbook
  task: read_from_postgres_task
  input:
    pg_user: '{{ get_pg_user.secret_value }}'
    pg_password: '{{ get_pg_password.secret_value }}'
    execution_id: '{{ workload.execution_id }}'
  next:
  - step: upload_csv_to_gs
- step: upload_csv_to_gs
  desc: Upload CSV file to Google Storage bucket
  type: workbook
  task: upload_csv_task
  input:
    key_id: '{{ get_gcs_key_id.secret_value }}'
    secret_key: '{{ get_gcs_secret_key.secret_value }}'
  next:
  - step: download_and_convert
- step: download_and_convert
  desc: Download CSV from GS and convert to Parquet
  type: workbook
  task: download_convert_task
  input:
    key_id: '{{ get_gcs_key_id.secret_value }}'
    secret_key: '{{ get_gcs_secret_key.secret_value }}'
  next:
  - step: create_postgres_table
- step: create_postgres_table
  desc: Create table in Postgres
  type: workbook
  task: create_table_task
  input:
    key_id: '{{ get_gcs_key_id.secret_value }}'
    secret_key: '{{ get_gcs_secret_key.secret_value }}'
  next:
  - step: load_data_to_postgres
- data:
    key_id: '{{ get_gcs_key_id.secret_value }}'
    secret_key: '{{ get_gcs_secret_key.secret_value }}'
  step: load_data_to_postgres
  desc: Load data from Parquet to Postgres
  type: workbook
  task: load_data_task
  next:
  - step: upload_parquet_to_gs
- data:
    key_id: '{{ get_gcs_key_id.secret_value }}'
    secret_key: '{{ get_gcs_secret_key.secret_value }}'
  step: upload_parquet_to_gs
  desc: Upload Parquet file to Google Storage bucket
  type: workbook
  task: upload_parquet_task
  next:
  - step: advanced_file_operations
- data:
    key_id: '{{ get_gcs_key_id.secret_value }}'
    secret_key: '{{ get_gcs_secret_key.secret_value }}'
  step: advanced_file_operations
  desc: Advanced file operations with Duckdb
  type: workbook
  task: advanced_file_operations_task
  next:
  - step: delete_secrets
- step: delete_secrets
  desc: Delete all secrets created during workflow
  type: workbook
  task: delete_secrets_task
  input:
    execution_id: '{{ workload.execution_id }}'
  next:
  - step: end
- step: end
  desc: End of workflow
workbook:
- name: setup_duckdb_task
  type: python
  code: "def main():\n    import os\n\n    # Get the data directory from environment\
    \ variable\n    data_dir = os.environ.get('NOETL_DATA_DIR', '/opt/noetl/data')\n\
    \    duckdb_dir = os.path.join(data_dir, 'noetldb')\n\n    # Create the directory\
    \ structure if it doesn't exist\n    os.makedirs(duckdb_dir, exist_ok=True)\n\n\
    \    return {\n        'status': 'success',\n        'directory_created': duckdb_dir,\n\
    \        'message': 'DuckDB directory structure created. The job package will\
    \ handle execution-specific database files.'\n    }\n"
- data:
    pg_user: demo
    pg_password: demo
    execution_id: '{{ execution_id }}'
    pg_host: db
    pg_port: '5432'
    pg_db: demo_noetl
  name: create_pg_secret_task
  type: duckdb
  command: "INSTALL postgres;\nLOAD postgres;\nCREATE OR REPLACE SECRET postgres_secret\
    \ (\n    TYPE POSTGRES,\n    HOST 'db',\n    PORT 5432,\n    DATABASE 'demo_noetl',\n\
    \    USER 'demo',\n    PASSWORD 'demo'\n);\n\n-- Test the secret by creating a\
    \ simple table\nCREATE TABLE test_pg_secret AS\nSELECT 'Postgres secret created\
    \ successfully' as status, CURRENT_TIMESTAMP as timestamp;\n"
- name: read_from_postgres_task
  type: duckdb
  input:
    key_id: '{{ key_id }}'
    secret_key: '{{ secret_key }}'
    pg_user: demo
    pg_password: demo
    execution_id: '{{ execution_id }}'
    db_host: db
    db_port: '5432'
    db_user: demo
    db_password: demo
    db_name: demo_noetl
  command: "-- Install and load Postgres extension\nINSTALL postgres;\nLOAD postgres;\n\
    INSTALL httpfs;\nLOAD httpfs;\nset s3_endpoint='storage.googleapis.com';\nset\
    \ s3_region='auto';\nset s3_url_style='path';\nset s3_use_ssl=true;\n-- set s3_access_key_id='{{\
    \ key_id }}';\n-- set s3_secret_access_key='{{ secret_key }}';\n\n\n\n-- Create\
    \ DuckDB secret for GCS access using S3-compatible interface\nCREATE OR REPLACE\
    \ SECRET s3_secret (\n    TYPE S3,\n    KEY_ID '{{ key_id }}',\n    SECRET '{{\
    \ secret_key }}'\n);\n-- Use the existing Postgres secret to attach database\n\
    ATTACH DATABASE 'postgres_secret' AS postgres_db (TYPE postgres);\n\n-- Read data\
    \ from test_data_table in the public schema (default)\n-- The table is in public\
    \ schema as confirmed by the error message\nDROP TABLE IF EXISTS temp_csv;\nCREATE\
    \ TABLE temp_csv AS\nSELECT * FROM postgres_db.{{ workload.source_table_name }};\n\
    \n-- Show the data\nSELECT * FROM temp_csv;\nDESCRIBE temp_csv;\n\n-- Save to\
    \ local CSV file\nCOPY temp_csv TO '{{ workload.local_csv_path }}' (FORMAT CSV,\
    \ HEADER);\n\n-- Clean up\nDROP TABLE temp_csv;\n"
- data:
    execution_id: '{{ execution_id }}'
  name: delete_secrets_task
  type: duckdb
  command: '-- Drop DuckDB secrets created during the workflow

    DROP SECRET IF EXISTS s3_secret;

    DROP SECRET IF EXISTS postgres_secret;


    -- Confirm secrets are removed

    SELECT ''All DuckDB secrets cleaned up'' as status;

    '
- name: create_gcs_secret_task
  type: duckdb
  input:
    key_id: '{{ key_id }}'
    secret_key: '{{ secret_key }}'
  command: "-- Install httpfs extension for cloud storage operations\nINSTALL httpfs;\n\
    LOAD httpfs;\n\n-- Drop existing secret if it exists\nDROP SECRET IF EXISTS gcs_secret;\n\
    set s3_endpoint='storage.googleapis.com';\nset s3_region='auto';\nset s3_url_style='path';\n\
    set s3_use_ssl=true;\n-- set s3_access_key_id='{{ key_id }}';\n-- set s3_secret_access_key='{{\
    \ secret_key }}';\n\n\n\n-- Create DuckDB secret for GCS access using S3-compatible\
    \ interface\nCREATE OR REPLACE SECRET s3_secret (\n    TYPE S3,\n    KEY_ID '{{\
    \ key_id }}',\n    SECRET '{{ secret_key }}',\n    ENDPOINT 'storage.googleapis.com'\n\
    );\n\n-- Test the secret by creating a simple table (drop first if exists)\nDROP\
    \ TABLE IF EXISTS test_gcs_secret;\nCREATE TABLE test_gcs_secret AS\nSELECT 'GCS\
    \ secret created successfully' as status, CURRENT_TIMESTAMP as timestamp;\nDROP\
    \ TABLE IF EXISTS gcs_test;\nCREATE TABLE gcs_test AS\nSELECT 'test' AS message,\
    \ CURRENT_TIMESTAMP AS timestamp;\n\n-- Test by uploading a small test file to\
    \ verify credentials work\n-- The s3_secret should be automatically used for GCS\
    \ operations\nCOPY gcs_test TO 'gs://{{ workload.gs_bucket }}/test_connection1.csv'\
    \ (FORMAT CSV, HEADER);\n\n-- Verify we can read it back\nSELECT 'GCS credentials\
    \ test successful' AS status, * FROM read_csv_auto('gs://{{ workload.gs_bucket\
    \ }}/test_connection1.csv');\n"
- name: test_gcs_credentials
  desc: Test GCS credentials using the created secret
  type: duckdb
  input:
    key_id: '{{ key_id }}'
    secret_key: '{{ secret_key }}'
  command: "INSTALL httpfs;\nLOAD httpfs;\n\n-- Drop existing secret if it exists\n\
    DROP SECRET IF EXISTS s3_secret;\nset s3_endpoint='storage.googleapis.com';\n\
    set s3_region='auto';\nset s3_url_style='path';\nset s3_use_ssl=true;\n\n-- Create\
    \ DuckDB secret for GCS access using S3-compatible interface\nCREATE OR REPLACE\
    \ SECRET s3_secret (\n    TYPE S3,\n    KEY_ID '{{ key_id }}',\n    SECRET '{{\
    \ secret_key }}',\n    ENDPOINT 'storage.googleapis.com'\n);\n\n-- Test the secret\
    \ by creating a simple table (drop first if exists)\nDROP TABLE IF EXISTS test_gcs_secret;\n\
    CREATE TABLE test_gcs_secret AS\nSELECT 'GCS secret created successfully' as status,\
    \ CURRENT_TIMESTAMP as timestamp;\nDROP TABLE IF EXISTS gcs_test;\nCREATE TABLE\
    \ gcs_test AS\nSELECT 'test' AS message, CURRENT_TIMESTAMP AS timestamp;\n\n--\
    \ Test by uploading a small test file to verify credentials work\n-- The s3_secret\
    \ should be automatically used for GCS operations\nCOPY gcs_test TO 'gs://{{ workload.gs_bucket\
    \ }}/test_connection.csv' (FORMAT CSV, HEADER);\n\n-- Verify we can read it back\n\
    SELECT 'GCS credentials test successful' AS status, * FROM read_csv_auto('gs://{{\
    \ workload.gs_bucket }}/test_connection.csv');\n\n-- Show the created secret to\
    \ verify it exists\nSHOW SECRETS;\n"
- data:
    key_id: '{{ key_id }}'
    secret_key: '{{ secret_key }}'
  name: upload_csv_task
  type: duckdb
  command: "-- Install and load necessary extensions\nINSTALL httpfs;\nLOAD httpfs;\n\
    set s3_endpoint='storage.googleapis.com';\nset s3_region='auto';\nset s3_url_style='path';\n\
    set s3_use_ssl=true;\nCREATE OR REPLACE SECRET s3_secret (\n    TYPE S3,\n   \
    \ KEY_ID '{{ key_id }}',\n    SECRET '{{ secret_key }}',\n    ENDPOINT 'storage.googleapis.com'\n\
    );\n-- The s3_secret will be automatically used for GCS operations\n-- Read CSV\
    \ file from local filesystem using auto-detection\nDROP TABLE IF EXISTS temp_csv;\n\
    CREATE TABLE temp_csv AS \nSELECT * FROM read_csv_auto('{{ workload.source_csv_path\
    \ }}', \n                           all_varchar=false,  \n                   \
    \        sample_size=-1);    \n\n-- Show the data and inferred types\nSELECT *\
    \ FROM temp_csv;\nDESCRIBE temp_csv;\n\n-- Upload to Google Storage using the\
    \ secret\nCOPY temp_csv TO 'gs://{{ workload.gs_bucket }}/{{ workload.gs_csv_path\
    \ }}' (FORMAT CSV, HEADER);\n\n-- Clean up\nDROP TABLE temp_csv;\n"
- data:
    key_id: '{{ key_id }}'
    secret_key: '{{ secret_key }}'
  name: download_convert_task
  type: duckdb
  command: "-- Install and load necessary extensions\nINSTALL httpfs;\nLOAD httpfs;\n\
    INSTALL parquet;\nLOAD parquet;\nINSTALL httpfs;\nLOAD httpfs;\nset s3_endpoint='storage.googleapis.com';\n\
    set s3_region='auto';\nset s3_url_style='path';\nset s3_use_ssl=true;\nCREATE\
    \ OR REPLACE SECRET s3_secret (\n    TYPE S3,\n    KEY_ID '{{ key_id }}',\n  \
    \  SECRET '{{ secret_key }}',\n    ENDPOINT 'storage.googleapis.com'\n);\n-- Download\
    \ CSV from Google Storage using the secret\nDROP TABLE IF EXISTS temp_csv;\nCREATE\
    \ TABLE temp_csv AS \nSELECT * FROM read_csv_auto('gs://{{ workload.gs_bucket\
    \ }}/{{ workload.gs_csv_path }}', \n                           sample_size=1000,\
    \    \n                           all_varchar=false);  \n\n-- Show the data and\
    \ inferred schema\nSELECT * FROM temp_csv;\nDESCRIBE temp_csv;\n\n-- Convert to\
    \ Parquet and save locally with compression\nCOPY temp_csv TO '{{ workload.local_parquet_path\
    \ }}' (\n  FORMAT PARQUET, \n  COMPRESSION ZSTD,\n  ROW_GROUP_SIZE 100000\n);\n\
    \n-- Clean up\nDROP TABLE temp_csv;\n"
- name: create_table_task
  type: postgres
  input:
    db_host: '{{ workload.pg_host }}'
    db_port: '{{ workload.pg_port }}'
    db_user: '{{ workload.pg_user }}'
    db_password: '{{ workload.pg_password }}'
    db_name: '{{ workload.pg_db }}'
  command: "-- SHOWCASE: This task uses direct authentication as an alternative to\
    \ secrets\n-- The following SQL demonstrates direct connection to Postgres\n--\
    \ without using secrets for authentication\n\n-- Drop table if it exists\nDROP\
    \ TABLE IF EXISTS {{ workload.table_name }};\n\n-- Create table with appropriate\
    \ columns based on test_data_table schema\nCREATE TABLE {{ workload.table_name\
    \ }} (\n  id INTEGER,\n  name VARCHAR(100),\n  age INTEGER,\n  created_at TIMESTAMP,\n\
    \  is_active BOOLEAN,\n  meta_data JSONB,\n  description TEXT\n);\n"
- name: load_data_task
  type: duckdb
  command: "-- Install and load extensions\nINSTALL parquet;\nLOAD parquet;\nINSTALL\
    \ postgres;\nLOAD postgres;\n\n-- Use the Postgres secret to attach database\n\
    ATTACH DATABASE 'postgres_secret' AS postgres_db (TYPE postgres);\n\n-- Load Parquet\
    \ file into DuckDB\nDROP TABLE IF EXISTS temp_parquet;\nCREATE TABLE temp_parquet\
    \ AS\nSELECT * FROM read_parquet('{{ workload.local_parquet_path }}',\n      \
    \                    binary_as_string=true,\n                          file_row_number=true);\n\
    \n-- Show the data and schema\nSELECT * FROM temp_parquet;\nDESCRIBE temp_parquet;\n\
    \n-- Get Parquet file metadata and schema\nSELECT * FROM parquet_metadata('{{\
    \ workload.local_parquet_path }}');\nSELECT * FROM parquet_schema('{{ workload.local_parquet_path\
    \ }}');\n\n-- Insert data into Postgres with proper type handling\nINSERT INTO\
    \ postgres_db.{{ workload.table_name }}\nSELECT\n  CASE WHEN id IS NULL THEN NULL\
    \ ELSE id::INTEGER END,\n  name,\n  CASE WHEN age IS NULL OR TRIM(age::VARCHAR)\
    \ = '' THEN NULL ELSE age::INTEGER END,\n  CURRENT_TIMESTAMP AS created_at,\n\
    \  TRUE AS is_active,\n  CASE WHEN meta_data IS NULL OR TRIM(meta_data::VARCHAR)\
    \ = '' THEN NULL ELSE meta_data::JSON END,\n  description\nFROM temp_parquet;\n\
    \n-- Verify data in Postgres\nSELECT * FROM postgres_db.{{ workload.table_name\
    \ }};\n\n-- Clean up\nDROP TABLE temp_parquet;\n"
- name: upload_parquet_task
  type: duckdb
  input:
    key_id: '{{ key_id }}'
    secret_key: '{{ secret_key }}'
    pg_user: demo
    pg_password: demo
    execution_id: '{{ execution_id }}'
    db_host: db
    db_port: '5432'
    db_user: demo
    db_password: demo
    db_name: demo_noetl
  command: "-- Install and load Postgres extension\nINSTALL postgres;\nLOAD postgres;\n\
    INSTALL httpfs;\nLOAD httpfs;\nINSTALL parquet;\nLOAD parquet;\nset s3_endpoint='storage.googleapis.com';\n\
    set s3_region='auto';\nset s3_url_style='path';\nset s3_use_ssl=true;\n\n-- Create\
    \ DuckDB secret for GCS access using S3-compatible interface\nCREATE OR REPLACE\
    \ SECRET s3_secret (\n    TYPE S3,\n    KEY_ID '{{ key_id }}',\n    SECRET '{{\
    \ secret_key }}',\n    ENDPOINT 'storage.googleapis.com'\n);\n\n-- Use the GCS\
    \ secret for all operations\n-- Example 1: Reading multiple files\nDROP TABLE\
    \ IF EXISTS sample_data;\nCREATE TABLE sample_data AS\nSELECT 1 AS id, 'Example\
    \ 1' AS name\nUNION ALL\nSELECT 2 AS id, 'Example 2' AS name;\n\nCOPY (SELECT\
    \ * FROM sample_data WHERE id = 1) TO '/tmp/sample1.csv' (FORMAT CSV, HEADER);\n\
    COPY (SELECT * FROM sample_data WHERE id = 2) TO '/tmp/sample2.csv' (FORMAT CSV,\
    \ HEADER);\n\nDROP TABLE IF EXISTS multi_file_read;\nCREATE TABLE multi_file_read\
    \ AS\nSELECT * FROM read_csv(['/tmp/sample1.csv', '/tmp/sample2.csv'], header\
    \ = true);\n\nSELECT 'Reading multiple files:' AS operation, * FROM multi_file_read;\n\
    \n-- Example 2: Reading compressed files\nCOPY sample_data TO '/tmp/sample_compressed.csv.gz'\
    \ (FORMAT CSV, HEADER);\n\nDROP TABLE IF EXISTS compressed_file_read;\nCREATE\
    \ TABLE compressed_file_read AS\nSELECT * FROM '/tmp/sample_compressed.csv.gz';\n\
    \nSELECT 'Reading compressed file:' AS operation, * FROM compressed_file_read;\n\
    \n-- Example 3: Using glob patterns\nDROP TABLE IF EXISTS glob_pattern_read;\n\
    CREATE TABLE glob_pattern_read AS\nSELECT * FROM read_csv('/tmp/sample*.csv',\
    \ header = true);\n\nSELECT 'Reading with glob pattern:' AS operation, * FROM\
    \ glob_pattern_read;\n\n-- Upload Parquet to Google Storage using the secret\n\
    COPY sample_data TO 'gs://{{ workload.gs_bucket }}/{{ workload.gs_parquet_path\
    \ }}' (FORMAT PARQUET);\n\n-- Clean up\nDROP TABLE sample_data;\nDROP TABLE multi_file_read;\n\
    DROP TABLE compressed_file_read;\nDROP TABLE glob_pattern_read;\n"
- name: advanced_file_operations_task
  type: duckdb
  command: '-- Install and load extensions

    INSTALL httpfs;

    LOAD httpfs;

    INSTALL parquet;

    LOAD parquet;


    -- Use the GCS secret for operations

    -- Create test data

    DROP TABLE IF EXISTS test_data;

    CREATE TABLE test_data AS

    SELECT 1 AS id, ''Test 1'' AS name, 100 AS value

    UNION ALL

    SELECT 2 AS id, ''Test 2'' AS name, 200 AS value;


    -- Save as CSV

    COPY test_data TO ''/tmp/advanced_test.csv'' (FORMAT CSV, HEADER);


    -- Read using auto-detection

    DROP TABLE IF EXISTS auto_read;

    CREATE TABLE auto_read AS

    SELECT * FROM read_csv_auto(''/tmp/advanced_test.csv'');


    SELECT ''Advanced operations complete:'' AS operation, * FROM auto_read;


    -- Clean up

    DROP TABLE test_data;

    DROP TABLE auto_read;

    '
