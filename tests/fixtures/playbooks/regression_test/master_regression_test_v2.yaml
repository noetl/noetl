apiVersion: noetl.io/v1
kind: Playbook
metadata:
  name: master_regression_test_v2
  path: tests/fixtures/playbooks/regression_test/master_regression_test_v2
  description: "Master regression test orchestrator - Async version with polling"
  version: "1.0"

workload:
  test_run_id: "{{ execution_id }}"
  pg_auth: pg_local
  max_wait_seconds: 300
  poll_interval: 5

workbook:
  - name: wait_for_execution
    desc: "Wait for a playbook execution to complete"
    tool: python
    code: |
      def main(context):
          """Wait for an execution to complete by polling"""
          import time
          import requests
          import os
          
          execution_id = context.get('execution_id')
          max_wait = context.get('max_wait_seconds', 300)
          poll_interval = context.get('poll_interval', 5)
          
          if not execution_id:
              return {
                  "status": "error",
                  "error": "No execution_id provided"
              }
          
          server_url = os.environ.get("NOETL_SERVER_URL", "http://localhost:8082").rstrip('/')
          if not server_url.endswith('/api'):
              server_url = server_url + '/api'
          status_url = f"{server_url}/executions/{execution_id}"
          
          start_time = time.time()
          
          while (time.time() - start_time) < max_wait:
              try:
                  response = requests.get(status_url, timeout=10)
                  if response.status_code == 200:
                      status_data = response.json()
                      status = status_data.get('status', 'unknown')
                      
                      # Check if completed or failed
                      if status in ['completed', 'failed', 'error']:
                          return {
                              "status": "success",
                              "execution_id": execution_id,
                              "final_status": status,
                              "wait_time": time.time() - start_time,
                              "status_data": status_data
                          }
                      
                      # Still running, wait and poll again
                      print(f"Execution {execution_id} status: {status}, waiting...")
                      time.sleep(poll_interval)
                  else:
                      print(f"Status check failed with code {response.status_code}, retrying...")
                      time.sleep(poll_interval)
              except Exception as e:
                  print(f"Error checking status: {e}, retrying...")
                  time.sleep(poll_interval)
          
          # Timeout
          return {
              "status": "error",
              "error": f"Timeout waiting for execution {execution_id} after {max_wait} seconds"
          }

  - name: fetch_execution_events
    desc: "Fetch events for a completed execution"
    tool: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      SELECT 
        node_name,
        event_type,
        status,
        error,
        result
      FROM noetl.event
      WHERE execution_id = {{ execution_id }}
      ORDER BY event_id

  - name: validate_test_result
    desc: "Validate a completed test execution"
    tool: python
    code: |
      def main(context):
          """Validate test execution results"""
          test_name = context.get('test_name', 'unknown')
          expected_status = context.get('expected_status', 'completed')
          execution_id = context.get('execution_id')
          final_status = context.get('final_status', 'unknown')
          events = context.get('events', [])
          
          # Basic validation
          status_match = (final_status == expected_status)
          
          # Count steps
          step_completed_events = [e for e in events if e.get('event_type') == 'step_completed']
          step_count = len(step_completed_events)
          
          # Check for errors
          error_events = [e for e in events if e.get('error')]
          has_errors = len(error_events) > 0
          
          test_passed = status_match and not has_errors
          
          return {
              "test_name": test_name,
              "execution_id": execution_id,
              "test_passed": test_passed,
              "validation_passed": status_match,
              "expected_status": expected_status,
              "actual_status": final_status,
              "step_count": step_count,
              "error_count": len(error_events),
              "errors": [e.get('error') for e in error_events] if has_errors else []
          }

workflow:
  - step: start
    desc: "Initialize regression test suite"
    tool: python
    code: |
      async def main():
          import datetime
          return {
              "status": "success",
              "data": {
                  "test_run_started": datetime.datetime.now().isoformat(),
                  "description": "NoETL Regression Test Suite - Async Version"
              }
          }
    next:
      - step: test_hello_world

  # === Test hello_world ===
  - step: test_hello_world
    desc: "Execute hello_world playbook"
    tool: playbook
    path: tests/fixtures/playbooks/hello_world
    vars:
      hello_world_execution_id: "{{ result.execution_id }}"
    next:
      - step: wait_hello_world

  - step: wait_hello_world
    desc: "Wait for hello_world to complete"
    tool: workbook
    name: wait_for_execution
    args:
      execution_id: "{{ vars.hello_world_execution_id }}"
      max_wait_seconds: "{{ workload.max_wait_seconds }}"
      poll_interval: "{{ workload.poll_interval }}"
    vars:
      hello_world_status: "{{ result.final_status }}"
    next:
      - step: fetch_hello_world_events

  - step: fetch_hello_world_events
    desc: "Fetch hello_world execution events"
    tool: workbook
    name: fetch_execution_events
    args:
      execution_id: "{{ vars.hello_world_execution_id }}"
    vars:
      hello_world_events: "{{ result }}"
    next:
      - step: validate_hello_world

  - step: validate_hello_world
    desc: "Validate hello_world result"
    tool: workbook
    name: validate_test_result
    args:
      test_name: hello_world
      expected_status: completed
      execution_id: "{{ vars.hello_world_execution_id }}"
      final_status: "{{ vars.hello_world_status }}"
      events: "{{ vars.hello_world_events }}"
    sink:
      tool: postgres
      auth: "{{ workload.pg_auth }}"
      table: noetl_test.regression_results
      data:
        test_run_id: "{{ workload.test_run_id }}"
        playbook_name: hello_world
        playbook_path: tests/fixtures/playbooks/hello_world
        category: basic
        execution_id: "{{ vars.hello_world_execution_id }}"
        status: "{{ vars.hello_world_status }}"
        test_passed: "{{ result.test_passed }}"
        validation_passed: "{{ result.validation_passed }}"
        actual_events: "{{ vars.hello_world_events | tojson }}"
        error_message: "{{ result.errors | tojson if result.errors else null }}"
        executed_at: now()
    next:
      - step: generate_summary

  # === Generate Summary ===
  - step: generate_summary
    desc: "Generate test run summary"
    tool: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      INSERT INTO noetl_test.regression_summary (
        test_run_id,
        total_tests,
        passed_tests,
        failed_tests,
        skipped_tests,
        success_rate,
        categories_tested
      )
      SELECT 
        {{ workload.test_run_id }},
        COUNT(*) as total_tests,
        COUNT(*) FILTER (WHERE test_passed = true) as passed_tests,
        COUNT(*) FILTER (WHERE test_passed = false) as failed_tests,
        0 as skipped_tests,
        CASE 
          WHEN COUNT(*) = 0 THEN 0
          ELSE ROUND(100.0 * COUNT(*) FILTER (WHERE test_passed = true) / COUNT(*), 2)
        END as success_rate,
        ARRAY_AGG(DISTINCT category) as categories_tested
      FROM noetl_test.regression_results
      WHERE test_run_id = {{ workload.test_run_id }}
      RETURNING *;
    next:
      - step: end

  - step: end
    desc: "Complete regression test"
    tool: python
    code: |
      async def main():
          return {
              "status": "success",
              "data": {"message": "Regression test completed"}
          }
