apiVersion: noetl.io/v1
kind: Playbook

metadata:
  name: http_to_databases
  path: tests/fixtures/playbooks/data_transfer/http_to_databases

workload:
  sf_auth: sf_test
  pg_auth: pg_local
  message: Fetch data from HTTP API and store in PostgreSQL, Snowflake, and DuckDB
  api_url: https://jsonplaceholder.typicode.com/users
  chunk_size: 1000

workflow:
  - step: start
    desc: Start HTTP to databases transfer pipeline
    next:
      - step: fetch_http_data

  - step: fetch_http_data
    desc: Fetch user data from JSONPlaceholder API
    tool: http
    method: GET
    url: "{{ workload.api_url }}"
    next:
      - step: create_sf_database

  - step: create_sf_database
    desc: Create Snowflake database and schema
    tool: snowflake
    auth: "{{ workload.sf_auth }}"
    command: |
      CREATE DATABASE IF NOT EXISTS TEST_DB;
      USE DATABASE TEST_DB;
      USE SCHEMA PUBLIC;
    next:
      - step: setup_pg_table

  - step: setup_pg_table
    desc: Create PostgreSQL table for HTTP data
    tool: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      CREATE TABLE IF NOT EXISTS public.http_users_pg (
        id INTEGER PRIMARY KEY,
        name TEXT,
        username TEXT,
        email TEXT,
        phone TEXT,
        website TEXT,
        company_name TEXT,
        city TEXT
      );
      
      TRUNCATE TABLE public.http_users_pg;
    next:
      - step: setup_sf_table

  - step: setup_sf_table
    desc: Create Snowflake table for HTTP data
    tool: snowflake
    auth: "{{ workload.sf_auth }}"
    command: |
      CREATE OR REPLACE TABLE HTTP_USERS_SF (
        id INTEGER PRIMARY KEY,
        name STRING,
        username STRING,
        email STRING,
        phone STRING,
        website STRING,
        company_name STRING,
        city STRING
      );
    next:
      - step: setup_duckdb_table

  - step: setup_duckdb_table
    desc: Create DuckDB table for HTTP data
    tool: duckdb
    command: |
      CREATE TABLE IF NOT EXISTS http_users_duckdb (
        id INTEGER PRIMARY KEY,
        name VARCHAR,
        username VARCHAR,
        email VARCHAR,
        phone VARCHAR,
        website VARCHAR,
        company_name VARCHAR,
        city VARCHAR
      );
      
      DELETE FROM http_users_duckdb;
    next:
      - step: transform_http_to_pg

  - step: transform_http_to_pg
    desc: Transform HTTP JSON data and prepare for database insertion
    tool: python
    code: |
      def main(input_data):
          """Transform HTTP API response and prepare for database insertion."""
          import json
          
          # Get the HTTP response - the key is the step name
          users = input_data.get('data', [])
          
          # Transform data for database insertion
          transformed_rows = []
          for user in users:
              if not user:
                  continue
              row = {
                  'id': user.get('id'),
                  'name': (user.get('name') or '').replace("'", "''"),  # Escape single quotes
                  'username': (user.get('username') or '').replace("'", "''"),
                  'email': (user.get('email') or '').replace("'", "''"),
                  'phone': (user.get('phone') or '').replace("'", "''"),
                  'website': (user.get('website') or '').replace("'", "''"),
                  'company_name': ((user.get('company') or {}).get('name') or '').replace("'", "''"),
                  'city': ((user.get('address') or {}).get('city') or '').replace("'", "''")
              }
              transformed_rows.append(row)
          
          return {
              'status': 'success',
              'rows': transformed_rows,
              'count': len(transformed_rows)
          }
    data:
      data: "{{ fetch_http_data.data }}"
    next:
      - step: insert_to_postgres

  - step: insert_to_postgres
    desc: Insert transformed data into PostgreSQL using iterator
    tool: iterator
    collection: "{{ transform_http_to_pg.rows }}"
    element: user
    mode: sequential
    workbook:
      - name: insert_user_pg
        tool: postgres
        auth: "{{ workload.pg_auth }}"
        command: |
          INSERT INTO public.http_users_pg (id, name, username, email, phone, website, company_name, city)
          VALUES (
            {{ user.id }},
            '{{ user.name }}',
            '{{ user.username }}',
            '{{ user.email }}',
            '{{ user.phone }}',
            '{{ user.website }}',
            '{{ user.company_name }}',
            '{{ user.city }}'
          )
          ON CONFLICT (id) DO UPDATE SET
            name = EXCLUDED.name,
            username = EXCLUDED.username,
            email = EXCLUDED.email,
            phone = EXCLUDED.phone,
            website = EXCLUDED.website,
            company_name = EXCLUDED.company_name,
            city = EXCLUDED.city;
    next:
      - step: verify_pg_data

  - step: verify_pg_data
    desc: Verify data in PostgreSQL
    tool: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      SELECT 
        COUNT(*) as total_users,
        COUNT(DISTINCT city) as unique_cities,
        COUNT(DISTINCT company_name) as unique_companies
      FROM public.http_users_pg;
    next:
      - step: insert_to_snowflake

  - step: insert_to_snowflake
    desc: Insert transformed data into Snowflake using iterator
    tool: iterator
    collection: "{{ transform_http_to_pg.rows }}"
    element: user
    mode: sequential
    workbook:
      - name: insert_user_sf
        tool: snowflake
        auth: "{{ workload.sf_auth }}"
        command: |
          MERGE INTO HTTP_USERS_SF AS target
          USING (SELECT 
            {{ user.id }} AS id,
            '{{ user.name }}' AS name,
            '{{ user.username }}' AS username,
            '{{ user.email }}' AS email,
            '{{ user.phone }}' AS phone,
            '{{ user.website }}' AS website,
            '{{ user.company_name }}' AS company_name,
            '{{ user.city }}' AS city
          ) AS source
          ON target.id = source.id
          WHEN MATCHED THEN
            UPDATE SET
              name = source.name,
              username = source.username,
              email = source.email,
              phone = source.phone,
              website = source.website,
              company_name = source.company_name,
              city = source.city
          WHEN NOT MATCHED THEN
            INSERT (id, name, username, email, phone, website, company_name, city)
            VALUES (source.id, source.name, source.username, source.email, source.phone, source.website, source.company_name, source.city);
    next:
      - step: verify_sf_data

  - step: verify_sf_data
    desc: Verify data in Snowflake
    tool: snowflake
    auth: "{{ workload.sf_auth }}"
    command: |
      SELECT 
        COUNT(*) as total_users,
        COUNT(DISTINCT city) as unique_cities,
        COUNT(DISTINCT company_name) as unique_companies
      FROM HTTP_USERS_SF;
    next:
      - step: insert_to_duckdb

  - step: insert_to_duckdb
    desc: Insert transformed data into DuckDB using iterator
    tool: iterator
    collection: "{{ transform_http_to_pg.rows }}"
    element: user
    mode: sequential
    workbook:
      - name: insert_user_duckdb
        tool: duckdb
        command: |
          INSERT INTO http_users_duckdb (id, name, username, email, phone, website, company_name, city)
          VALUES (
            {{ user.id }},
            '{{ user.name }}',
            '{{ user.username }}',
            '{{ user.email }}',
            '{{ user.phone }}',
            '{{ user.website }}',
            '{{ user.company_name }}',
            '{{ user.city }}'
          )
          ON CONFLICT (id) DO UPDATE SET
            name = EXCLUDED.name,
            username = EXCLUDED.username,
            email = EXCLUDED.email,
            phone = EXCLUDED.phone,
            website = EXCLUDED.website,
            company_name = EXCLUDED.company_name,
            city = EXCLUDED.city;
    next:
      - step: verify_duckdb_data

  - step: verify_duckdb_data
    desc: Verify data in DuckDB
    tool: duckdb
    command: |
      SELECT 
        COUNT(*) as total_users,
        COUNT(DISTINCT city) as unique_cities,
        COUNT(DISTINCT company_name) as unique_companies
      FROM http_users_duckdb;
    next:
      - step: cross_verify

  - step: cross_verify
    desc: Cross-verify data across all three databases
    tool: python
    code: |
      def main(input_data):
          """Compare results across PostgreSQL, Snowflake, and DuckDB."""
          
          pg_result = input_data.get('verify_pg_data', {}).get('command_0', {}).get('rows', [{}])[0]
          sf_result = input_data.get('verify_sf_data', {}).get('statement_0', {}).get('result', [{}])[0]
          duckdb_result = input_data.get('verify_duckdb_data', {}).get('result', [{}])[0]
          
          comparison = {
              'postgresql': {
                  'total_users': pg_result.get('total_users', 0),
                  'unique_cities': pg_result.get('unique_cities', 0),
                  'unique_companies': pg_result.get('unique_companies', 0)
              },
              'snowflake': {
                  'total_users': sf_result.get('TOTAL_USERS', 0),
                  'unique_cities': sf_result.get('UNIQUE_CITIES', 0),
                  'unique_companies': sf_result.get('UNIQUE_COMPANIES', 0)
              },
              'duckdb': {
                  'total_users': duckdb_result.get('total_users', 0),
                  'unique_cities': duckdb_result.get('unique_cities', 0),
                  'unique_companies': duckdb_result.get('unique_companies', 0)
              }
          }
          
          # Check if all databases have the same counts
          pg_count = comparison['postgresql']['total_users']
          sf_count = comparison['snowflake']['total_users']
          duckdb_count = comparison['duckdb']['total_users']
          
          all_match = (pg_count == sf_count == duckdb_count)
          
          return {
              'status': 'success' if all_match else 'warning',
              'comparison': comparison,
              'all_databases_match': all_match,
              'message': f'All databases have {pg_count} users' if all_match else 'Database counts do not match'
          }
    next:
      - step: cleanup

  - step: cleanup
    desc: Clean up test tables (optional - comment out to keep data)
    tool: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      -- DROP TABLE IF EXISTS public.http_users_pg;
      -- Commented out to keep data for inspection
      SELECT 'Cleanup skipped - tables preserved for inspection' as message;
    next:
      - step: end

  - step: end
    desc: End HTTP to databases transfer pipeline
