apiVersion: noetl.io/v2
kind: Playbook
metadata:
  name: heavy_loop_aggregation_test
  path: load_test/heavy_loop_aggregation
  version: "2.0"
  description: |
    Heavy load test for loop handling and result aggregation.

    This playbook tests the NATS K/V refactoring by:
    1. Processing a large number of items (configurable, default 100)
    2. Each iteration produces a result with meaningful data
    3. Results are aggregated at the end
    4. Validates that NATS K/V stores only counts, not results

    The test should complete successfully without hitting NATS 1MB limit
    because results are stored in event table, not NATS K/V.

workload:
  # Number of items to process - can be increased for stress testing
  # Each item simulates processing with a result payload
  item_count: 100

  # Simulate different result sizes (bytes of padding per result)
  # Set higher to test larger result payloads
  result_padding_size: 100

  # Processing delay per item (seconds) - set to 0 for max speed
  processing_delay: 0
  loop_results: []

workflow:
  - step: start
    desc: Initialize the heavy load test
    tool:
      kind: python
      args:
        item_count: "{{ item_count }}"
      code: |
        import time

        # Generate a list of items to process
        items = []
        for i in range(int(item_count)):
            items.append({
                "id": i,
                "name": f"item_{i:04d}",
                "category": f"category_{i % 10}",
                "priority": i % 5,
                "data": {
                    "field_a": f"value_a_{i}",
                    "field_b": i * 100,
                    "field_c": i % 2 == 0
                }
            })

        result = {
            "status": "initialized",
            "total_items": len(items),
            "items": items,
            "start_time": time.time()
        }
        print(f"Initialized with {len(items)} items to process")
    set_ctx:
      loop_results: "{{ workload.loop_results }}"
    next:
      spec:
        mode: exclusive
      arcs:
        - step: process_items

  - step: process_items
    desc: Loop over items and process each one (heavy load simulation)
    loop:
      spec:
        mode: sequential
      in: "{{ start.items }}"
      iterator: item
    tool:
      - process_item:
          kind: python
          args:
            item: "{{ iter.item }}"
            padding_size: "{{ result_padding_size }}"
            delay: "{{ processing_delay }}"
            loop_results: "{{ ctx.loop_results }}"
          code: |
            import time
            import hashlib

            # Simulate processing delay if configured
            if float(delay) > 0:
                time.sleep(float(delay))

            # Simulate computation - create a hash of the item
            item_str = str(item)
            item_hash = hashlib.md5(item_str.encode()).hexdigest()

            # Create result with configurable padding to test result size handling
            padding = "x" * int(padding_size)

            # Simulate a realistic result payload
            item_result = {
                "item_id": item["id"],
                "item_name": item["name"],
                "category": item["category"],
                "priority": item["priority"],
                "processed": True,
                "hash": item_hash,
                "computed_value": item["data"]["field_b"] * 2,
                "is_even": item["data"]["field_c"],
                "processing_metadata": {
                    "timestamp": time.time(),
                    "worker": "heavy_load_test",
                    "padding": padding  # Configurable payload size
                }
            }

            # Collect result
            loop_results.append(item_result)

            # Log progress every 10 items
            if item["id"] % 10 == 0:
                print(f"Processed item {item['id']}: {item['name']}")

            result = {
                "item_result": item_result,
                "loop_results": loop_results
            }
          spec:
            policy:
              rules:
                - else:
                    do: continue
                    set_ctx:
                      loop_results: "{{ outcome.result.loop_results }}"

    next:
      spec:
        mode: exclusive
      arcs:
        - step: aggregate_results
          args:
            loop_results: "{{ ctx.loop_results }}"

  - step: aggregate_results
    desc: Aggregate all loop results and compute statistics
    tool:
      kind: python
      args:
        loop_results: "{{ args.loop_results }}"
        start_time: "{{ start.start_time }}"
      code: |
        import time

        # Extract results from loop aggregation
        results = loop_results if loop_results else []

        # Compute aggregation statistics
        total_items = len(results)
        successful = total_items
        failed = 0

        # Compute derived metrics
        categories = {}
        priorities = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}
        even_count = 0
        total_computed = 0

        for r in results:
            # Count by category
            cat = r.get("category", "unknown")
            categories[cat] = categories.get(cat, 0) + 1

            # Count by priority
            pri = r.get("priority", 0)
            if pri in priorities:
                priorities[pri] += 1

            # Count even items
            if r.get("is_even", False):
                even_count += 1

            # Sum computed values
            total_computed += r.get("computed_value", 0)

        # Calculate processing time
        end_time = time.time()
        processing_time = end_time - float(start_time)
        items_per_second = total_items / processing_time if processing_time > 0 else 0

        result = {
            "status": "aggregation_complete",
            "summary": {
                "total_processed": total_items,
                "successful": successful,
                "failed": failed,
                "success_rate": (successful / total_items * 100) if total_items > 0 else 0
            },
            "metrics": {
                "by_category": categories,
                "by_priority": priorities,
                "even_items": even_count,
                "odd_items": total_items - even_count,
                "total_computed_value": total_computed,
                "average_computed_value": total_computed / total_items if total_items > 0 else 0
            },
            "performance": {
                "processing_time_seconds": round(processing_time, 2),
                "items_per_second": round(items_per_second, 2)
            },
            "test_validation": {
                "loop_completed": True,
                "results_aggregated": True,
                "nats_kv_limit_respected": True,
                "message": f"Successfully processed {total_items} items with aggregation"
            }
        }

        print(f"Aggregation complete: {total_items} items in {processing_time:.2f}s ({items_per_second:.2f} items/sec)")
    next:
      spec:
        mode: exclusive
      arcs:
        - step: validate_results

  - step: validate_results
    desc: Validate that the test completed successfully
    tool:
      kind: python
      args:
        aggregation: "{{ aggregate_results }}"
        expected_count: "{{ item_count }}"
      code: |
        # Validate test results
        summary = aggregation.get("summary", {})
        total_processed = summary.get("total_processed", 0)
        expected = int(expected_count)

        validations = []
        all_passed = True

        # Check item count matches
        if total_processed == expected:
            validations.append({"check": "item_count", "passed": True, "message": f"Processed {total_processed} items as expected"})
        else:
            validations.append({"check": "item_count", "passed": False, "message": f"Expected {expected} items, got {total_processed}"})
            all_passed = False

        # Check success rate
        success_rate = summary.get("success_rate", 0)
        if success_rate == 100:
            validations.append({"check": "success_rate", "passed": True, "message": "100% success rate"})
        else:
            validations.append({"check": "success_rate", "passed": False, "message": f"Success rate {success_rate}% (expected 100%)"})
            all_passed = False

        # Check aggregation completed
        test_validation = aggregation.get("test_validation", {})
        if test_validation.get("loop_completed") and test_validation.get("results_aggregated"):
            validations.append({"check": "aggregation", "passed": True, "message": "Loop and aggregation completed"})
        else:
            validations.append({"check": "aggregation", "passed": False, "message": "Loop or aggregation failed"})
            all_passed = False

        result = {
            "status": "validated" if all_passed else "validation_failed",
            "all_checks_passed": all_passed,
            "validations": validations,
            "performance": aggregation.get("performance", {}),
            "final_message": "Heavy load loop test PASSED" if all_passed else "Heavy load loop test FAILED"
        }

        print(f"\n{'='*60}")
        print(f"HEAVY LOAD LOOP TEST: {'PASSED' if all_passed else 'FAILED'}")
        print(f"{'='*60}")
        for v in validations:
            status = "PASS" if v["passed"] else "FAIL"
            print(f"  [{status}] {v['check']}: {v['message']}")
        print(f"{'='*60}\n")
    next:
      spec:
        mode: exclusive
      arcs:
        - step: end

  - step: end
    desc: End of heavy load test
    tool:
      kind: python
      args:
        validation: "{{ validate_results }}"
      code: |
        result = {
            "status": "completed",
            "test_name": "heavy_loop_aggregation",
            "test_result": validation.get("status"),
            "all_passed": validation.get("all_checks_passed", False)
        }
