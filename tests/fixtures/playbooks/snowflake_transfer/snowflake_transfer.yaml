apiVersion: noetl.io/v1
kind: Playbook

metadata:
  name: snowflake_transfer
  path: tests/fixtures/playbooks/snowflake_transfer

workload:
  sf_auth: sf_test
  pg_auth: pg_local
  message: Snowflake <-> PostgreSQL data transfer with chunked streaming
  chunk_size: 1000
  test_table_sf: TEST_DATA
  test_table_pg: public.test_snowflake_transfer

workflow:
  - step: start
    desc: Start Snowflake transfer pipeline
    next:
      - step: setup_pg_table

  - step: setup_pg_table
    desc: Create PostgreSQL target table for Snowflake data
    type: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      CREATE TABLE IF NOT EXISTS {{ workload.test_table_pg }} (
        id INTEGER PRIMARY KEY,
        name TEXT,
        value NUMERIC(10,2),
        created_at TIMESTAMPTZ,
        metadata JSONB
      );
      
      TRUNCATE TABLE {{ workload.test_table_pg }};
    next:
      - step: setup_sf_table

  - step: setup_sf_table
    desc: Create and populate Snowflake source table
    type: snowflake
    auth: "{{ workload.sf_auth }}"
    command: |
      CREATE OR REPLACE TABLE {{ workload.test_table_sf }} (
        id INTEGER PRIMARY KEY,
        name STRING,
        value NUMERIC(10,2),
        created_at TIMESTAMP_TZ,
        metadata VARIANT
      );
      
      -- Insert test data
      INSERT INTO {{ workload.test_table_sf }} VALUES
        (1, 'Record 1', 100.50, CURRENT_TIMESTAMP(), PARSE_JSON('{"type": "test", "batch": 1}')),
        (2, 'Record 2', 200.75, CURRENT_TIMESTAMP(), PARSE_JSON('{"type": "test", "batch": 1}')),
        (3, 'Record 3', 300.25, CURRENT_TIMESTAMP(), PARSE_JSON('{"type": "test", "batch": 2}')),
        (4, 'Record 4', 400.00, CURRENT_TIMESTAMP(), PARSE_JSON('{"type": "test", "batch": 2}')),
        (5, 'Record 5', 500.99, CURRENT_TIMESTAMP(), PARSE_JSON('{"type": "test", "batch": 3}'));
    next:
      - step: transfer_sf_to_pg

  - step: transfer_sf_to_pg
    desc: Transfer data from Snowflake to PostgreSQL in chunks
    type: python
    code: |
      import psycopg
      from noetl.plugin.snowflake import execute_snowflake_transfer_task
      from jinja2 import Environment
      
      def main(input_data):
          """Transfer data from Snowflake to PostgreSQL using chunked streaming"""
          
          # Get parameters from input
          execution_id = input_data.get('execution_id')
          sf_auth = input_data.get('sf_auth')
          pg_auth = input_data.get('pg_auth')
          chunk_size = input_data.get('chunk_size', 1000)
          
          # Task configuration
          task_config = {
              'transfer_direction': 'sf_to_pg',
              'source_query': 'SELECT * FROM TEST_DATA ORDER BY id',
              'target_table': 'public.test_snowflake_transfer',
              'chunk_size': chunk_size,
              'mode': 'append'
          }
          
          # Connection parameters (these would come from credentials in real usage)
          task_with = {
              'sf_account': input_data.get('sf_account'),
              'sf_user': input_data.get('sf_user'),
              'sf_password': input_data.get('sf_password'),
              'sf_warehouse': input_data.get('sf_warehouse', 'COMPUTE_WH'),
              'sf_database': input_data.get('sf_database'),
              'sf_schema': input_data.get('sf_schema', 'PUBLIC'),
              'pg_host': input_data.get('pg_host', 'localhost'),
              'pg_port': input_data.get('pg_port', '5432'),
              'pg_user': input_data.get('pg_user'),
              'pg_password': input_data.get('pg_password'),
              'pg_database': input_data.get('pg_database')
          }
          
          context = {'execution_id': execution_id}
          jinja_env = Environment()
          
          # Execute transfer
          result = execute_snowflake_transfer_task(
              task_config=task_config,
              context=context,
              jinja_env=jinja_env,
              task_with=task_with
          )
          
          return result
    data:
      execution_id: "{{ execution_id }}"
      sf_auth: "{{ workload.sf_auth }}"
      pg_auth: "{{ workload.pg_auth }}"
      chunk_size: "{{ workload.chunk_size }}"
      sf_account: "{{ secret.SF_ACCOUNT }}"
      sf_user: "{{ secret.SF_USER }}"
      sf_password: "{{ secret.SF_PASSWORD }}"
      sf_warehouse: "{{ secret.SF_WAREHOUSE | default('COMPUTE_WH') }}"
      sf_database: "{{ secret.SF_DATABASE }}"
      sf_schema: "{{ secret.SF_SCHEMA | default('PUBLIC') }}"
      pg_host: "{{ secret.PG_HOST | default('localhost') }}"
      pg_port: "{{ secret.PG_PORT | default('5432') }}"
      pg_user: "{{ secret.PG_USER }}"
      pg_password: "{{ secret.PG_PASSWORD }}"
      pg_database: "{{ secret.PG_DATABASE }}"
    next:
      - step: verify_pg_data

  - step: verify_pg_data
    desc: Verify data was transferred to PostgreSQL
    type: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      SELECT 
        COUNT(*) as row_count,
        MIN(id) as min_id,
        MAX(id) as max_id,
        SUM(value) as total_value
      FROM {{ workload.test_table_pg }};
    next:
      - step: setup_pg_source

  - step: setup_pg_source
    desc: Create PostgreSQL source table for reverse transfer
    type: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      CREATE TABLE IF NOT EXISTS public.test_pg_source (
        id INTEGER PRIMARY KEY,
        description TEXT,
        amount NUMERIC(10,2),
        status TEXT
      );
      
      TRUNCATE TABLE public.test_pg_source;
      
      INSERT INTO public.test_pg_source VALUES
        (101, 'PG Record 1', 1000.00, 'active'),
        (102, 'PG Record 2', 2000.50, 'pending'),
        (103, 'PG Record 3', 3000.75, 'active'),
        (104, 'PG Record 4', 4000.25, 'completed'),
        (105, 'PG Record 5', 5000.99, 'active');
    next:
      - step: setup_sf_target

  - step: setup_sf_target
    desc: Create Snowflake target table for reverse transfer
    type: snowflake
    auth: "{{ workload.sf_auth }}"
    command: |
      CREATE OR REPLACE TABLE PG_IMPORTED_DATA (
        id INTEGER PRIMARY KEY,
        description STRING,
        amount NUMERIC(10,2),
        status STRING
      );
    next:
      - step: transfer_pg_to_sf

  - step: transfer_pg_to_sf
    desc: Transfer data from PostgreSQL to Snowflake in chunks
    type: python
    code: |
      import psycopg
      from noetl.plugin.snowflake import execute_snowflake_transfer_task
      from jinja2 import Environment
      
      def main(input_data):
          """Transfer data from PostgreSQL to Snowflake using chunked streaming"""
          
          # Get parameters from input
          execution_id = input_data.get('execution_id')
          chunk_size = input_data.get('chunk_size', 1000)
          
          # Task configuration
          task_config = {
              'transfer_direction': 'pg_to_sf',
              'source_query': 'SELECT * FROM public.test_pg_source ORDER BY id',
              'target_table': 'PG_IMPORTED_DATA',
              'chunk_size': chunk_size,
              'mode': 'append'
          }
          
          # Connection parameters
          task_with = {
              'sf_account': input_data.get('sf_account'),
              'sf_user': input_data.get('sf_user'),
              'sf_password': input_data.get('sf_password'),
              'sf_warehouse': input_data.get('sf_warehouse', 'COMPUTE_WH'),
              'sf_database': input_data.get('sf_database'),
              'sf_schema': input_data.get('sf_schema', 'PUBLIC'),
              'pg_host': input_data.get('pg_host', 'localhost'),
              'pg_port': input_data.get('pg_port', '5432'),
              'pg_user': input_data.get('pg_user'),
              'pg_password': input_data.get('pg_password'),
              'pg_database': input_data.get('pg_database')
          }
          
          context = {'execution_id': execution_id}
          jinja_env = Environment()
          
          # Execute transfer
          result = execute_snowflake_transfer_task(
              task_config=task_config,
              context=context,
              jinja_env=jinja_env,
              task_with=task_with
          )
          
          return result
    data:
      execution_id: "{{ execution_id }}"
      chunk_size: "{{ workload.chunk_size }}"
      sf_account: "{{ secret.SF_ACCOUNT }}"
      sf_user: "{{ secret.SF_USER }}"
      sf_password: "{{ secret.SF_PASSWORD }}"
      sf_warehouse: "{{ secret.SF_WAREHOUSE | default('COMPUTE_WH') }}"
      sf_database: "{{ secret.SF_DATABASE }}"
      sf_schema: "{{ secret.SF_SCHEMA | default('PUBLIC') }}"
      pg_host: "{{ secret.PG_HOST | default('localhost') }}"
      pg_port: "{{ secret.PG_PORT | default('5432') }}"
      pg_user: "{{ secret.PG_USER }}"
      pg_password: "{{ secret.PG_PASSWORD }}"
      pg_database: "{{ secret.PG_DATABASE }}"
    next:
      - step: verify_sf_data

  - step: verify_sf_data
    desc: Verify data was transferred to Snowflake
    type: snowflake
    auth: "{{ workload.sf_auth }}"
    command: |
      SELECT 
        COUNT(*) as row_count,
        MIN(id) as min_id,
        MAX(id) as max_id,
        SUM(amount) as total_amount
      FROM PG_IMPORTED_DATA;
    next:
      - step: cleanup

  - step: cleanup
    desc: Clean up test tables
    type: postgres
    auth: "{{ workload.pg_auth }}"
    command: |
      DROP TABLE IF EXISTS {{ workload.test_table_pg }};
      DROP TABLE IF EXISTS public.test_pg_source;
    next:
      - step: end

  - step: end
    desc: End Snowflake transfer test pipeline
