apiVersion: noetl.io/v2
kind: Playbook
metadata:
  name: postgres_excel_gcs_test
  path: tests/postgres_excel_gcs
  description: UPDATED VERSION Test postgres temp tables -> Excel multi-sheet -> GCS upload

workload:
  pg_auth: pg_local
  gcs_auth: gcs_service_account
  gcs_bucket: noetl-demo-output
  output_filename: test_export

workflow:
  - step: start
    desc: Create and populate 3 persistent tables with dummy data
    tool:
      kind: postgres
      auth: "{{ workload.pg_auth }}"
      command: |
        -- Create schema for test tables
        CREATE SCHEMA IF NOT EXISTS test_export;
        
        -- Drop tables if they exist
        DROP TABLE IF EXISTS test_export.employees CASCADE;
        DROP TABLE IF EXISTS test_export.products CASCADE;
        DROP TABLE IF EXISTS test_export.orders CASCADE;
        
        -- Create persistent table 1: employees
        CREATE TABLE test_export.employees (
          emp_id INT,
          emp_name VARCHAR(100),
          department VARCHAR(50)
        );
        
        INSERT INTO test_export.employees VALUES 
          (1, 'Alice Johnson', 'Engineering'),
          (2, 'Bob Smith', 'Sales'),
          (3, 'Carol Williams', 'Marketing');
        
        -- Create persistent table 2: products
        CREATE TABLE test_export.products (
          product_id INT,
          product_name VARCHAR(100),
          price DECIMAL(10,2)
        );
        
        INSERT INTO test_export.products VALUES 
          (101, 'Laptop', 999.99),
          (102, 'Mouse', 29.99),
          (103, 'Keyboard', 79.99);
        
        -- Create persistent table 3: orders
        CREATE TABLE test_export.orders (
          order_id INT,
          customer_name VARCHAR(100),
          order_date DATE
        );
        
        INSERT INTO test_export.orders VALUES 
          (1001, 'John Doe', '2025-01-15'),
          (1002, 'Jane Smith', '2025-01-16'),
          (1003, 'Mike Brown', '2025-01-17');
        
        SELECT 'Tables created successfully' as status;
    next:
      - step: create_excel_and_upload

  - step: create_excel_and_upload
    desc: Export tables to Excel and upload to GCS via sink
    tool:
      kind: python
      code: |
        def main(pg_auth, output_filename):
            """
            1. Connect to Postgres and export tables to CSV
            2. Create multi-sheet Excel from CSVs
            3. Return Excel path for GCS sink to upload
            """
            import duckdb
            import xlsxwriter
            import csv
            import os
            import httpx
            import json
            
            # Fetch Postgres credential
            server_url = os.environ.get('NOETL_SERVER_URL', 'http://noetl.noetl.svc.cluster.local:8080')
            if not server_url.endswith('/api'):
                server_url = server_url + '/api'
            
            url = f'{server_url}/credentials/{pg_auth}?include_data=true'
            response = httpx.get(url, timeout=5.0)
            response.raise_for_status()
            
            body = response.json() or {}
            print(f"[DEBUG] Credential response body: {body}", flush=True)
            
            cred_data = body.get('data', {})
            print(f"[DEBUG] Initial cred_data: {cred_data}", flush=True)
            
            # Handle double-nested structure: {data: {data: {actual_fields}}}
            while isinstance(cred_data, dict) and 'data' in cred_data and isinstance(cred_data['data'], dict):
                cred_data = cred_data['data']
                print(f"[DEBUG] Unwrapped cred_data: {cred_data}", flush=True)
            
            print(f"[DEBUG] Final cred_data keys: {list(cred_data.keys()) if isinstance(cred_data, dict) else type(cred_data)}", flush=True)
            
            # Build Postgres connection string (handle both db_* and direct field names)
            host = cred_data.get('db_host') or cred_data.get('host', 'localhost')
            port = cred_data.get('db_port') or cred_data.get('port', 5432)
            user = cred_data.get('db_user') or cred_data.get('user', 'postgres')
            password = cred_data.get('db_password') or cred_data.get('password', '')
            database = cred_data.get('db_name') or cred_data.get('database', 'postgres')
            
            conn_str = f"postgresql://{user}:{password}@{host}:{port}/{database}"
            
            # Connect to Postgres via DuckDB and export CSVs
            conn = duckdb.connect(':memory:')
            conn.execute("INSTALL postgres; LOAD postgres;")
            conn.execute(f"ATTACH '{conn_str}' AS pg (TYPE postgres);")
            
            csv_files = []
            tables = [
                ('test_export.employees', 'Employees', 'emp_id'),
                ('test_export.products', 'Products', 'product_id'),
                ('test_export.orders', 'Orders', 'order_id')
            ]
            
            for table, sheet_name, sort_col in tables:
                csv_path = f'/tmp/{output_filename}_{sheet_name.lower()}.csv'
                conn.execute(f"COPY (SELECT * FROM pg.{table} ORDER BY {sort_col}) TO '{csv_path}' (HEADER, DELIMITER ',')")
                csv_files.append((csv_path, sheet_name))
            
            conn.close()
            
            # Create Excel workbook
            excel_path = f'/tmp/{output_filename}.xlsx'
            workbook = xlsxwriter.Workbook(excel_path)
            
            header_format = workbook.add_format({
                'bold': True,
                'bg_color': '#D9E1F2',
                'border': 1,
                'align': 'center'
            })
            
            # Process each CSV into Excel sheet
            for csv_path, sheet_name in csv_files:
                worksheet = workbook.add_worksheet(sheet_name)
                
                with open(csv_path, 'r') as f:
                    reader = csv.reader(f)
                    for row_idx, row in enumerate(reader):
                        for col_idx, value in enumerate(row):
                            if row_idx == 0:
                                worksheet.write(row_idx, col_idx, value, header_format)
                                worksheet.set_column(col_idx, col_idx, 20)
                            else:
                                worksheet.write(row_idx, col_idx, value)
                
                # Clean up CSV
                os.remove(csv_path)
            
            workbook.close()
            file_size = os.path.getsize(excel_path)
            
            return {
                'status': 'success',
                'local_path': excel_path,
                'sheets_created': len(csv_files),
                'file_size_bytes': file_size
            }
    args:
      pg_auth: "{{ workload.pg_auth }}"
      output_filename: "{{ workload.output_filename }}"
    case:
      - when: "{{ event.name == 'call.done' and response.status == 'success' }}"
        then:
          sink:
            tool:
              kind: gcs
              source: "{{ response.local_path }}"
              destination: "gs://{{ workload.gcs_bucket }}/exports/{{ workload.output_filename }}.xlsx"
              credential: "{{ workload.gcs_auth }}"
              content_type: "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
          next:
            - step: cleanup

  - step: cleanup
    desc: Cleanup test tables
    tool:
      kind: postgres
      auth: "{{ workload.pg_auth }}"
      command: |
        DROP TABLE IF EXISTS test_export.employees CASCADE;
        DROP TABLE IF EXISTS test_export.products CASCADE;
        DROP TABLE IF EXISTS test_export.orders CASCADE;
        DROP SCHEMA IF EXISTS test_export CASCADE;
        SELECT 'Cleanup completed' as status;
    next:
      - step: end

  - step: end
    desc: Test completed
    tool:
      kind: python
      code: |
        def main():
            return {
                'status': 'completed',
                'message': 'Successfully created Excel from postgres tables and uploaded to GCS'
            }
