apiVersion: noetl.io/v2
kind: Playbook
metadata:
  name: postgres_excel_gcs_test
  path: tests/postgres_excel_gcs
  description: Test postgres tables -> CSV via DuckDB to GCS -> Excel to GCS

workload:
  pg_auth: pg_k8s
  gcs_credential: gcs_bhs_state_program_hmac
  gcs_service_account: gcs_service_account
  gcs_bucket: bhs-state-program-reports2-dev
  output_filename: test_export_{{ execution_id }}
keychain:
  - name: gcs_service_account_keychain
    kind: credential
    scope: local
    credential: gcs_service_account

workflow:
  - step: start
    desc: Create and populate 3 persistent tables with dummy data
    tool:
      kind: postgres
      auth: "{{ workload.pg_auth }}"
      command: |
        -- CREATE SCHEMA IF NOT EXISTS test_export;
        DROP TABLE IF EXISTS employees CASCADE;
        DROP TABLE IF EXISTS products CASCADE;
        DROP TABLE IF EXISTS orders CASCADE;
        
        CREATE TABLE employees (emp_id INT, emp_name VARCHAR(100), department VARCHAR(50));
        INSERT INTO employees VALUES (1, 'Alice Johnson', 'Engineering'), (2, 'Bob Smith', 'Sales'), (3, 'Carol Williams', 'Marketing');
        
        CREATE TABLE products (product_id INT, product_name VARCHAR(100), price DECIMAL(10,2));
        INSERT INTO products VALUES (101, 'Laptop', 999.99), (102, 'Mouse', 29.99), (103, 'Keyboard', 79.99);
        
        CREATE TABLE orders (order_id INT, customer_name VARCHAR(100), order_date DATE);
        INSERT INTO orders VALUES (1001, 'John Doe', '2025-01-15'), (1002, 'Jane Smith', '2025-01-16'), (1003, 'Mike Brown', '2025-01-17');
        
        SELECT 'Tables created' as status;
    next:
      - step: export_to_gcs

  - step: export_to_gcs
    desc: Query data, export CSV to GCS, create Excel, and upload to GCS
    tool:
      kind: duckdb
      auth:
        pg_db:
          source: credential
          tool: postgres
          key: "{{ workload.pg_auth }}"
        gcs_secret:
          source: credential
          tool: hmac
          key: "{{ workload.gcs_credential }}"
          scope: gs://{{ workload.gcs_bucket }}
      commands: |
        INSTALL postgres; LOAD postgres;
        INSTALL httpfs; LOAD httpfs;
        ATTACH '' AS pg_db (TYPE postgres, SECRET pg_db);
        
        -- Export CSV files to GCS
        COPY (SELECT * FROM pg_db.public.employees ORDER BY emp_id) TO 'gs://{{ workload.gcs_bucket }}/exports/{{ workload.output_filename }}_employees.csv' (FORMAT CSV, HEADER TRUE);
        COPY (SELECT * FROM pg_db.public.products ORDER BY product_id) TO 'gs://{{ workload.gcs_bucket }}/exports/{{ workload.output_filename }}_products.csv' (FORMAT CSV, HEADER TRUE);
        COPY (SELECT * FROM pg_db.public.orders ORDER BY order_id) TO 'gs://{{ workload.gcs_bucket }}/exports/{{ workload.output_filename }}_orders.csv' (FORMAT CSV, HEADER TRUE);
    next:
      - step: create_excel

  - step: create_excel
    desc: Create Excel file from CSV files
    tool:
      kind: python
      auth:
        gcs:
          service: gcs_service_account
          source: credential
          tool: service_account
          key: "{{ workload.gcs_service_account }}"
      libs:
        pd: pandas
        storage:
          from: google.cloud
          import: storage
        io: io
        os: os
      args:
        bucket: "{{ workload.gcs_bucket }}"
        prefix: "exports/{{ workload.output_filename }}"
        output_filename: "{{ workload.output_filename }}"
        execution_id: "{{ execution_id }}"
      code: |
        # Pure Python code - no imports, no def main()
        # Variables injected: bucket, prefix, output_filename, execution_id
        # Libraries imported: pd, storage, io, os

        csv_files = {
          "Employees": f"{prefix}_employees.csv",
          "Products": f"{prefix}_products.csv",
          "Orders": f"{prefix}_orders.csv",
        }

        client = storage.Client()
        bucket_name = bucket
        bucket_obj = client.bucket(bucket_name)
        output_path = f"/tmp/{output_filename}_{execution_id}.xlsx"

        sheets_summary = {}
        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
          for sheet_name, csv_blob_path in csv_files.items():
            blob = bucket_obj.blob(csv_blob_path)
            csv_text = blob.download_as_text()
            df = pd.read_csv(io.StringIO(csv_text))
            df.to_excel(writer, sheet_name=sheet_name, index=False)
            sheets_summary[sheet_name] = len(df)

        print(f"Created Excel file: {output_path}")
        for sheet_name, row_count in sheets_summary.items():
          print(f"  - {sheet_name}: {row_count} rows")

        workbook_blob_name = f"exports/{output_filename}.xlsx"
        workbook_blob = bucket_obj.blob(workbook_blob_name)
        workbook_blob.upload_from_filename(output_path)

        gcs_uri = f"gs://{bucket_obj.name}/{workbook_blob_name}"
        print(f"Uploaded to GCS: {gcs_uri}")

        os.remove(output_path)

        result = {
          "gcs_uri": gcs_uri,
          "local_path": output_path,
          "sheets": sheets_summary
        }
    next:
      - step: cleanup

  - step: cleanup
    desc: Cleanup test tables
    tool:
      kind: postgres
      auth: "{{ workload.pg_auth }}"
      command: |
        DROP TABLE IF EXISTS employees CASCADE;
        DROP TABLE IF EXISTS products CASCADE;
        DROP TABLE IF EXISTS orders CASCADE;
        -- DROP SCHEMA IF EXISTS test_export CASCADE;
        SELECT 'Cleanup completed' as status;
    next:
      - step: end

  - step: end
    desc: Test completed
    tool:
      kind: python
      auth: {}
      libs: {}
      args: {}
      code: |
        result = {'status': 'completed', 'message': 'CSV and Excel uploaded to GCS', 'csv_count': 3, 'excel_count': 1}