apiVersion: noetl.io/v2
kind: Playbook
metadata:
  name: multi_playbook_batch
  path: batch_execution/multi_playbook_batch

workload:
  jobId: '{{ job.uuid }}'
  secret_name: test-secret
  environment: dev
  GOOGLE_CLOUD_PROJECT: noetl-demo-19700101
  cities:
    - name: London
      lat: 51.51
      lon: -0.13
  base_url: https://api.open-meteo.com/v1
  temperature_threshold: 26
  baseFilePath: /opt/noetl/data/test
  bucket: test-bucket
  pg_host: postgres.postgres.svc.cluster.local
  pg_port: '5432'
  pg_user: demo
  pg_password: demo
  pg_db: demo_noetl

workflow:
  - step: start
    desc: Start Multiple Playbook Batch Workflow
    tool:
      kind: python
      args: {}
      code: |
        result = {"status": "initialized", "message": "Starting batch workflow"}
    next:
      spec:
        mode: exclusive
      arcs:
        - step: run_http_test

  - step: run_http_test
    desc: Run HTTP test playbook
    tool:
      kind: playbook
      path: tests/fixtures/playbooks/data_transfer/http_to_postgres_simple
      payload:
        url: https://httpbin.org/get
        method: GET
    next:
      spec:
        mode: exclusive
      arcs:
        - step: run_weather_processing
          args:
            http_result: '{{ run_http_test }}'

  - step: run_weather_processing
    desc: Run weather processing playbook
    tool:
      kind: playbook
      path: tests/fixtures/playbooks/control_flow_workbook
      payload:
        cities: '{{ cities }}'
        base_url: '{{ base_url }}'
        temperature_threshold: '{{ temperature_threshold }}'
    next:
      spec:
        mode: exclusive
      arcs:
        - step: run_data_transformation
          args:
            weather_result: '{{ run_weather_processing }}'

  - step: run_data_transformation
    desc: Run data transformation playbook
    tool:
      kind: playbook
      path: tests/fixtures/playbooks/duckdb_gcs_workload_identity/workload_identity
      payload:
        baseFilePath: '{{ baseFilePath }}'
        bucket: '{{ bucket }}'
        pg_host: '{{ pg_host }}'
        pg_port: '{{ pg_port }}'
        pg_user: '{{ pg_user }}'
        pg_password: '{{ pg_password }}'
        pg_db: '{{ pg_db }}'
    next:
      spec:
        mode: exclusive
      arcs:
        - step: store_results

  - step: store_results
    desc: Store the results from all playbooks
    tool:
      kind: python
      libs:
        duckdb: duckdb
        json: json
        time: time
      args:
        http_result: '{{ run_http_test }}'
        weather_result: '{{ run_weather_processing }}'
        data_result: '{{ run_data_transformation }}'
        pg_host: '{{ pg_host }}'
        pg_port: '{{ pg_port }}'
        pg_user: '{{ pg_user }}'
        pg_password: '{{ pg_password }}'
        pg_db: '{{ pg_db }}'
        execution_id: '{{ job.uuid }}'
      code: |
        # Convert results to JSON strings
        http_json = json.dumps(http_result) if isinstance(http_result, dict) else str(http_result)
        weather_json = json.dumps(weather_result) if isinstance(weather_result, dict) else str(weather_result)
        data_json = json.dumps(data_result) if isinstance(data_result, dict) else str(data_result)

        # Create DuckDB connection
        conn = duckdb.connect()

        try:
            # Install and load required extensions
            conn.execute("INSTALL postgres")
            conn.execute("LOAD postgres")
            conn.execute("INSTALL json")
            conn.execute("LOAD json")

            # Attach Postgres database
            attach_query = f"ATTACH 'dbname={pg_db} user={pg_user} password={pg_password} host={pg_host} port={pg_port}' AS postgres_db (TYPE postgres)"
            try:
                conn.execute(attach_query)
            except Exception as e:
                if "already attached" not in str(e):
                    raise e

            # Create local DuckDB table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS playbook_batch_results (
                    id BIGINT,
                    execution_id VARCHAR,
                    http_result TEXT,
                    weather_result TEXT,
                    data_result TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Insert into local DuckDB table
            current_time_ms = int(time.time() * 1000)
            conn.execute("""
                INSERT INTO playbook_batch_results (
                    id, execution_id, http_result, weather_result, data_result
                ) VALUES (?, ?, ?, ?, ?)
            """, [current_time_ms, execution_id, http_json, weather_json, data_json])

            # Create Postgres table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS postgres_db.playbook_batch_results (
                    id BIGINT PRIMARY KEY,
                    execution_id VARCHAR,
                    http_result TEXT,
                    weather_result TEXT,
                    data_result TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # Insert into Postgres table
            conn.execute("""
                INSERT INTO postgres_db.playbook_batch_results (
                    id, execution_id, http_result, weather_result, data_result
                ) VALUES (?, ?, ?, ?, ?)
            """, [current_time_ms + 1, execution_id, http_json, weather_json, data_json])

            # Verify insertion
            query_result = conn.execute("SELECT COUNT(*) FROM playbook_batch_results").fetchone()
            total_records = query_result[0] if query_result else 0

            result = {
                "status": "success",
                "message": "Batch results stored successfully",
                "total_records": total_records,
                "execution_id": execution_id
            }

        except Exception as e:
            result = {
                "status": "error",
                "message": f"Failed to store batch results: {str(e)}",
                "execution_id": execution_id
            }
        finally:
            conn.close()
    next:
      spec:
        mode: exclusive
      arcs:
        - step: end

  - step: end
    desc: End of batch workflow
    tool:
      kind: python
      args: {}
      code: |
        result = {"status": "completed"}
