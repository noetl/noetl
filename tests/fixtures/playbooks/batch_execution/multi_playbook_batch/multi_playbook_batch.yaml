apiVersion: noetl.io/v1
kind: Playbook
metadata:
  name: multi_playbook_batch
  path: batch_execution/multi_playbook_batch

workload:
  jobId: '{{ job.uuid }}'
  secret_name: test-secret
  environment: dev
  GOOGLE_CLOUD_PROJECT: noetl-demo-19700101
  cities:
    - name: London
      lat: 51.51
      lon: -0.13
  base_url: https://api.open-meteo.com/v1
  temperature_threshold: 26
  baseFilePath: /opt/noetl/data/test
  bucket: test-bucket
  pg_host: postgres.postgres.svc.cluster.local
  pg_port: '5432'
  pg_user: demo
  pg_password: demo
  pg_db: demo_noetl
workflow:
  - step: start
    desc: Start Multiple Playbook Batch Workflow
    next:
      - step: run_http_test
  
  - step: run_http_test
    desc: Run HTTP test playbook
    tool: playbook
    path: tests/fixtures/playbooks/data_transfer/http_to_postgres_simple
    payload:
      url: https://httpbin.org/get
      method: GET
    next:
      - step: run_weather_processing
        args:
          http_result: '{{ run_http_test }}'
  
  - step: run_weather_processing
    desc: Run weather processing playbook
    tool: playbook
    path: tests/fixtures/playbooks/control_flow_workbook
    payload:
      cities: '{{ workload.cities }}'
      base_url: '{{ workload.base_url }}'
      temperature_threshold: '{{ workload.temperature_threshold }}'
    next:
      - step: run_data_transformation
        args:
          weather_result: '{{ run_weather_processing }}'
  
  - step: run_data_transformation
    desc: Run data transformation playbook
    tool: playbook
    path: tests/fixtures/playbooks/duckdb_gcs_workload_identity/workload_identity
    payload:
      baseFilePath: '{{ workload.baseFilePath }}'
      bucket: '{{ workload.bucket }}'
      pg_host: '{{ workload.pg_host }}'
      pg_port: '{{ workload.pg_port }}'
      pg_user: '{{ workload.pg_user }}'
      pg_password: '{{ workload.pg_password }}'
      pg_db: '{{ workload.pg_db }}'
    next:
      - step: store_results
  - step: store_results
    desc: Store the results from all playbooks
    tool: python
    args:
      http_result: '{{ run_http_test }}'
      weather_result: '{{ run_weather_processing }}'
      data_result: '{{ run_data_transformation }}'
      pg_host: '{{ workload.pg_host }}'
      pg_port: '{{ workload.pg_port }}'
      pg_user: '{{ workload.pg_user }}'
      pg_password: '{{ workload.pg_password }}'
      pg_db: '{{ workload.pg_db }}'
    code: |
      def main(http_result, weather_result, data_result, pg_host, pg_port, pg_user, pg_password, pg_db, execution_id):
          import duckdb
          import json
          import time
          
          # Convert results to JSON strings
          http_json = json.dumps(http_result) if isinstance(http_result, dict) else str(http_result)
          weather_json = json.dumps(weather_result) if isinstance(weather_result, dict) else str(weather_result)
          data_json = json.dumps(data_result) if isinstance(data_result, dict) else str(data_result)
          
          # Create DuckDB connection
          conn = duckdb.connect()
          
          try:
              # Install and load required extensions
              conn.execute("INSTALL postgres")
              conn.execute("LOAD postgres")
              conn.execute("INSTALL json")
              conn.execute("LOAD json")
              
              # Attach Postgres database
              attach_query = f"ATTACH 'dbname={pg_db} user={pg_user} password={pg_password} host={pg_host} port={pg_port}' AS postgres_db (TYPE postgres)"
              try:
                  conn.execute(attach_query)
              except Exception as e:
                  if "already attached" not in str(e):
                      raise e
              
              # Create local DuckDB table
              conn.execute("""
                  CREATE TABLE IF NOT EXISTS playbook_batch_results (
                      id BIGINT,
                      execution_id VARCHAR,
                      http_result TEXT,
                      weather_result TEXT,
                      data_result TEXT,
                      timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                  )
              """)
              
              # Insert into local DuckDB table
              current_time_ms = int(time.time() * 1000)
              conn.execute("""
                  INSERT INTO playbook_batch_results (
                      id, execution_id, http_result, weather_result, data_result
                  ) VALUES (?, ?, ?, ?, ?)
              """, [current_time_ms, execution_id, http_json, weather_json, data_json])
              
              # Create Postgres table
              conn.execute("""
                  CREATE TABLE IF NOT EXISTS postgres_db.playbook_batch_results (
                      id BIGINT PRIMARY KEY,
                      execution_id VARCHAR,
                      http_result TEXT,
                      weather_result TEXT,
                      data_result TEXT,
                      timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                  )
              """)
              
              # Insert into Postgres table
              conn.execute("""
                  INSERT INTO postgres_db.playbook_batch_results (
                      id, execution_id, http_result, weather_result, data_result
                  ) VALUES (?, ?, ?, ?, ?)
              """, [current_time_ms + 1, execution_id, http_json, weather_json, data_json])
              
              # Verify insertion
              result = conn.execute("SELECT COUNT(*) FROM playbook_batch_results").fetchone()
              total_records = result[0] if result else 0
              
              return {
                  "status": "success",
                  "message": "Batch results stored successfully",
                  "total_records": total_records,
                  "execution_id": execution_id
              }
              
          except Exception as e:
              return {
                  "status": "error",
                  "message": f"Failed to store batch results: {str(e)}",
                  "execution_id": execution_id
              }
          finally:
              conn.close()
    next:
      - step: end
  
  - step: end
    desc: End of batch workflow