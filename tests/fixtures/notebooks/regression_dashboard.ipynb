{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da71209e",
   "metadata": {},
   "source": [
    "# NoETL Regression Test Dashboard\n",
    "\n",
    "Comprehensive regression testing dashboard for NoETL system using modern data tools:\n",
    "- **psycopg3** for PostgreSQL connections\n",
    "- **DuckDB** for analytics and aggregations\n",
    "- **Polars** for high-performance data manipulation\n",
    "- **PyArrow** for efficient data transfer\n",
    "- **Plotly** for interactive visualizations\n",
    "\n",
    "**Features:**\n",
    "- Master regression test execution\n",
    "- Real-time execution monitoring\n",
    "- Event analysis and validation\n",
    "- Performance metrics and visualizations\n",
    "- Error detection and debugging\n",
    "- Historical trend analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ccf137",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Data processing imports - modern stack\n",
    "import psycopg  # psycopg3\n",
    "import duckdb\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Visualization imports\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configuration from Kubernetes\n",
    "DB_CONFIG = {\n",
    "    \"host\": os.getenv(\"POSTGRES_HOST\", \"postgres.postgres.svc.cluster.local\"),\n",
    "    \"port\": os.getenv(\"POSTGRES_PORT\", \"5432\"),\n",
    "    \"user\": os.getenv(\"POSTGRES_USER\", \"demo\"),\n",
    "    \"password\": os.getenv(\"POSTGRES_PASSWORD\", \"demo\"),\n",
    "    \"dbname\": os.getenv(\"POSTGRES_DB\", \"demo_noetl\")\n",
    "}\n",
    "\n",
    "# NoETL server configuration\n",
    "NOETL_SERVER_URL = os.getenv(\n",
    "    \"NOETL_SERVER_URL\", \n",
    "    \"http://noetl-server.noetl.svc.cluster.local:8080\"\n",
    ")\n",
    "\n",
    "# Test configuration\n",
    "MASTER_TEST_PATH = \"tests/fixtures/playbooks/regression_test/master_regression_test\"\n",
    "EXPECTED_STEPS = 53\n",
    "POLL_INTERVAL = 5  # seconds\n",
    "MAX_WAIT_TIME = 300  # seconds\n",
    "\n",
    "print(\"\u2713 Configuration loaded\")\n",
    "print(f\"  Server: {NOETL_SERVER_URL}\")\n",
    "print(f\"  Database: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}\")\n",
    "print(f\"  Expected steps: {EXPECTED_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00501e",
   "metadata": {},
   "source": [
    "## 2. Database Connection Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32cc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postgres_connection():\n",
    "    \"\"\"Get psycopg3 connection to NoETL database\"\"\"\n",
    "    conn_string = f\"host={DB_CONFIG['host']} port={DB_CONFIG['port']} \" \\\n",
    "                  f\"dbname={DB_CONFIG['dbname']} user={DB_CONFIG['user']} \" \\\n",
    "                  f\"password={DB_CONFIG['password']}\"\n",
    "    return psycopg.connect(conn_string)\n",
    "\n",
    "def query_to_polars(query: str) -> pl.DataFrame:\n",
    "    \"\"\"Execute PostgreSQL query and return as Polars DataFrame\"\"\"\n",
    "    with get_postgres_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            columns = [desc[0] for desc in cur.description]\n",
    "            data = cur.fetchall()\n",
    "    return pl.DataFrame(data, schema=columns)\n",
    "\n",
    "def query_to_arrow(query: str) -> pa.Table:\n",
    "    \"\"\"Execute PostgreSQL query and return as PyArrow Table\"\"\"\n",
    "    with get_postgres_connection() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            columns = [desc[0] for desc in cur.description]\n",
    "            data = cur.fetchall()\n",
    "            return pa.Table.from_pydict(\n",
    "                {col: [row[i] for row in data] for i, col in enumerate(columns)}\n",
    "            )\n",
    "\n",
    "def init_duckdb_with_postgres():\n",
    "    \"\"\"Initialize DuckDB with PostgreSQL connection\"\"\"\n",
    "    conn = duckdb.connect(':memory:')\n",
    "    \n",
    "    # Install and load postgres extension\n",
    "    conn.execute(\"INSTALL postgres\")\n",
    "    conn.execute(\"LOAD postgres\")\n",
    "    \n",
    "    # Attach PostgreSQL database\n",
    "    attach_query = f\"\"\"\n",
    "        ATTACH 'dbname={DB_CONFIG['dbname']} user={DB_CONFIG['user']} \n",
    "        password={DB_CONFIG['password']} host={DB_CONFIG['host']} \n",
    "        port={DB_CONFIG['port']}' AS noetl_db (TYPE postgres)\n",
    "    \"\"\"\n",
    "    conn.execute(attach_query)\n",
    "    \n",
    "    return conn\n",
    "\n",
    "# Test connections\n",
    "try:\n",
    "    with get_postgres_connection() as conn:\n",
    "        print(\"\u2713 PostgreSQL connection successful\")\n",
    "    \n",
    "    duck_conn = init_duckdb_with_postgres()\n",
    "    result = duck_conn.execute(\"SELECT COUNT(*) FROM noetl_db.noetl.event\").fetchone()\n",
    "    print(f\"\u2713 DuckDB connection successful (total events: {result[0]:,})\")\n",
    "    duck_conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bc0af",
   "metadata": {},
   "source": [
    "## 3. Execute Master Regression Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_regression_test() -> Dict:\n",
    "    \"\"\"Start master regression test execution\"\"\"\n",
    "    url = f\"{NOETL_SERVER_URL}/api/run/playbook\"\n",
    "    payload = {\"path\": MASTER_TEST_PATH}\n",
    "    \n",
    "    print(f\"Starting regression test...\")\n",
    "    response = requests.post(url, json=payload, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    result = response.json()\n",
    "    execution_id = result['execution_id']\n",
    "    \n",
    "    print(f\"\u2713 Test started: execution_id = {execution_id}\")\n",
    "    print(f\"  Status: {result['status']}\")\n",
    "    print(f\"  Start time: {result['start_time']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Start the test\n",
    "test_result = start_regression_test()\n",
    "EXECUTION_ID = test_result['execution_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cc344",
   "metadata": {},
   "source": [
    "## 4. Real-Time Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_execution_status(execution_id: int) -> pl.DataFrame:\n",
    "    \"\"\"Get current execution status with event counts\"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            event_type,\n",
    "            COUNT(*) as count,\n",
    "            MAX(created_at) as last_event_time\n",
    "        FROM noetl.event\n",
    "        WHERE execution_id = {execution_id}\n",
    "        GROUP BY event_type\n",
    "        ORDER BY event_type\n",
    "    \"\"\"\n",
    "    return query_to_polars(query)\n",
    "\n",
    "def monitor_execution(execution_id: int, max_wait: int = MAX_WAIT_TIME):\n",
    "    \"\"\"Monitor execution until completion or timeout\"\"\"\n",
    "    start_time = time.time()\n",
    "    last_step_count = 0\n",
    "    \n",
    "    print(f\"Monitoring execution {execution_id}...\")\n",
    "    print(f\"{'Time':<8} {'Steps':<8} {'Status':<20} {'Events'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    while (time.time() - start_time) < max_wait:\n",
    "        status_df = get_execution_status(execution_id)\n",
    "        \n",
    "        # Extract metrics\n",
    "        step_completed = status_df.filter(pl.col('event_type') == 'step_completed')['count'].to_list()\n",
    "        step_count = step_completed[0] if step_completed else 0\n",
    "        \n",
    "        playbook_completed = status_df.filter(pl.col('event_type') == 'playbook_completed')['count'].to_list()\n",
    "        is_complete = len(playbook_completed) > 0 and playbook_completed[0] > 0\n",
    "        \n",
    "        playbook_failed = status_df.filter(pl.col('event_type') == 'playbook_failed')['count'].to_list()\n",
    "        is_failed = len(playbook_failed) > 0 and playbook_failed[0] > 0\n",
    "        \n",
    "        # Print update if progress changed\n",
    "        if step_count != last_step_count or is_complete or is_failed:\n",
    "            elapsed = int(time.time() - start_time)\n",
    "            status = \"COMPLETED\" if is_complete else (\"FAILED\" if is_failed else \"RUNNING\")\n",
    "            total_events = status_df['count'].sum()\n",
    "            \n",
    "            print(f\"{elapsed:<8} {step_count:>3}/{EXPECTED_STEPS:<3} {status:<20} {total_events:>6}\")\n",
    "            last_step_count = step_count\n",
    "        \n",
    "        # Check completion\n",
    "        if is_complete:\n",
    "            print(f\"\\n\u2713 Test completed successfully in {int(time.time() - start_time)} seconds\")\n",
    "            return True\n",
    "        elif is_failed:\n",
    "            print(f\"\\n\u2717 Test failed after {int(time.time() - start_time)} seconds\")\n",
    "            return False\n",
    "        \n",
    "        time.sleep(POLL_INTERVAL)\n",
    "    \n",
    "    print(f\"\\n\u26a0 Timeout after {max_wait} seconds\")\n",
    "    return False\n",
    "\n",
    "# Monitor the test\n",
    "test_success = monitor_execution(EXECUTION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd94db9",
   "metadata": {},
   "source": [
    "## 5. Execution Analysis with DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b79df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DuckDB for analytics\n",
    "ddb = init_duckdb_with_postgres()\n",
    "\n",
    "# Comprehensive event analysis\n",
    "analysis_query = f\"\"\"\n",
    "WITH event_summary AS (\n",
    "    SELECT\n",
    "        event_type,\n",
    "        COUNT(*) as event_count,\n",
    "        MIN(created_at) as first_event,\n",
    "        MAX(created_at) as last_event,\n",
    "        COUNT(DISTINCT node_name) as unique_nodes\n",
    "    FROM noetl_db.noetl.event\n",
    "    WHERE execution_id = {EXECUTION_ID}\n",
    "    GROUP BY event_type\n",
    "),\n",
    "timing AS (\n",
    "    SELECT\n",
    "        MIN(created_at) as start_time,\n",
    "        MAX(created_at) as end_time,\n",
    "        EXTRACT(EPOCH FROM (MAX(created_at) - MIN(created_at))) as duration_seconds\n",
    "    FROM noetl_db.noetl.event\n",
    "    WHERE execution_id = {EXECUTION_ID}\n",
    ")\n",
    "SELECT \n",
    "    e.*,\n",
    "    t.duration_seconds,\n",
    "    ROUND(CAST(e.event_count AS DOUBLE) / NULLIF(t.duration_seconds, 0), 2) as events_per_second\n",
    "FROM event_summary e\n",
    "CROSS JOIN timing t\n",
    "ORDER BY e.event_count DESC\n",
    "\"\"\"\n",
    "\n",
    "analysis_df = ddb.execute(analysis_query).pl()\n",
    "print(\"\\n\ud83d\udcca Event Analysis:\")\n",
    "print(analysis_df)\n",
    "\n",
    "# Step-by-step timing analysis\n",
    "step_timing_query = f\"\"\"\n",
    "SELECT\n",
    "    node_name,\n",
    "    MIN(CASE WHEN event_type = 'step_started' THEN created_at END) as start_time,\n",
    "    MAX(CASE WHEN event_type = 'step_completed' THEN created_at END) as end_time,\n",
    "    EXTRACT(EPOCH FROM (\n",
    "        MAX(CASE WHEN event_type = 'step_completed' THEN created_at END) -\n",
    "        MIN(CASE WHEN event_type = 'step_started' THEN created_at END)\n",
    "    )) as duration_seconds\n",
    "FROM noetl_db.noetl.event\n",
    "WHERE execution_id = {EXECUTION_ID}\n",
    "    AND node_name IS NOT NULL\n",
    "    AND event_type IN ('step_started', 'step_completed')\n",
    "GROUP BY node_name\n",
    "HAVING MAX(CASE WHEN event_type = 'step_completed' THEN created_at END) IS NOT NULL\n",
    "ORDER BY start_time\n",
    "\"\"\"\n",
    "\n",
    "step_timing_df = ddb.execute(step_timing_query).pl()\n",
    "print(\"\\n\u23f1\ufe0f  Step Timing (Top 10 slowest):\")\n",
    "print(step_timing_df.sort('duration_seconds', descending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad75c7",
   "metadata": {},
   "source": [
    "## 6. Validation and Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_regression_test(execution_id: int) -> Dict:\n",
    "    \"\"\"Comprehensive validation of regression test results\"\"\"\n",
    "    validation = {'execution_id': execution_id, 'passed': True, 'issues': [], 'metrics': {}}\n",
    "    \n",
    "    # Check 1: Execution completed\n",
    "    completed_count = ddb.execute(f\"\"\"\n",
    "        SELECT COUNT(*) FROM noetl_db.noetl.event\n",
    "        WHERE execution_id = {execution_id} AND event_type = 'playbook_completed'\n",
    "    \"\"\").fetchone()[0]\n",
    "    validation['metrics']['playbook_completed'] = completed_count\n",
    "    if completed_count == 0:\n",
    "        validation['passed'] = False\n",
    "        validation['issues'].append('Playbook did not complete')\n",
    "    \n",
    "    # Check 2: Expected number of steps\n",
    "    step_count = ddb.execute(f\"\"\"\n",
    "        SELECT COUNT(DISTINCT node_name) FROM noetl_db.noetl.event\n",
    "        WHERE execution_id = {execution_id} AND event_type = 'step_completed'\n",
    "    \"\"\").fetchone()[0]\n",
    "    validation['metrics']['steps_completed'] = step_count\n",
    "    validation['metrics']['expected_steps'] = EXPECTED_STEPS\n",
    "    if step_count != EXPECTED_STEPS:\n",
    "        validation['passed'] = False\n",
    "        validation['issues'].append(f'Expected {EXPECTED_STEPS} steps, got {step_count}')\n",
    "    \n",
    "    # Check 3: Performance metrics\n",
    "    perf = ddb.execute(f\"\"\"\n",
    "        SELECT COUNT(*) as total_events, COUNT(DISTINCT event_type) as event_types,\n",
    "               EXTRACT(EPOCH FROM (MAX(created_at) - MIN(created_at))) as duration\n",
    "        FROM noetl_db.noetl.event WHERE execution_id = {execution_id}\n",
    "    \"\"\").fetchone()\n",
    "    validation['metrics'].update({\n",
    "        'total_events': perf[0],\n",
    "        'event_types': perf[1],\n",
    "        'total_duration_seconds': round(perf[2], 2),\n",
    "        'events_per_second': round(perf[0] / perf[2], 2) if perf[2] else 0\n",
    "    })\n",
    "    \n",
    "    return validation\n",
    "\n",
    "# Run validation\n",
    "validation_result = validate_regression_test(EXECUTION_ID)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\ud83d\udccb VALIDATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nExecution ID: {validation_result['execution_id']}\")\n",
    "print(f\"Status: {'\u2713 PASSED' if validation_result['passed'] else '\u2717 FAILED'}\")\n",
    "print(\"\\n\ud83d\udcca Metrics:\")\n",
    "for key, value in validation_result['metrics'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "if validation_result['issues']:\n",
    "    print(\"\\n\u26a0\ufe0f  Issues:\")\n",
    "    for issue in validation_result['issues']:\n",
    "        print(f\"  - {issue}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87be261",
   "metadata": {},
   "source": [
    "## 7. Error Detection and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(execution_id: int):\n",
    "    \"\"\"Detailed error analysis and debugging information\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"\ud83d\udd0d ERROR ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get all error-related events\n",
    "    error_query = f\"\"\"\n",
    "        SELECT event_id, event_type, node_name, status, created_at, result, meta\n",
    "        FROM noetl_db.noetl.event\n",
    "        WHERE execution_id = {execution_id}\n",
    "            AND (event_type LIKE '%failed%' OR event_type LIKE '%error%'\n",
    "                 OR status = 'FAILED' OR status = 'ERROR')\n",
    "        ORDER BY created_at\n",
    "    \"\"\"\n",
    "    error_df = ddb.execute(error_query).pl()\n",
    "    \n",
    "    if len(error_df) == 0:\n",
    "        print(\"\\n\u2713 No errors detected\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n\u26a0\ufe0f  Found {len(error_df)} error events\\n\")\n",
    "    \n",
    "    # Check for recovery\n",
    "    print(\"\ud83d\udd04 Retry/Recovery Analysis:\")\n",
    "    recovery_query = f\"\"\"\n",
    "        WITH failures AS (\n",
    "            SELECT node_name, event_type, created_at as failure_time\n",
    "            FROM noetl_db.noetl.event\n",
    "            WHERE execution_id = {execution_id} AND event_type IN ('action_failed', 'step_failed')\n",
    "        ),\n",
    "        successes AS (\n",
    "            SELECT node_name, event_type, created_at as success_time\n",
    "            FROM noetl_db.noetl.event\n",
    "            WHERE execution_id = {execution_id} AND event_type IN ('action_completed', 'step_completed')\n",
    "        )\n",
    "        SELECT f.node_name, COUNT(*) as failure_count,\n",
    "               MAX(s.success_time) as final_success_time,\n",
    "               CASE WHEN MAX(s.success_time) > MAX(f.failure_time) THEN 'RECOVERED' ELSE 'FAILED' END as status\n",
    "        FROM failures f LEFT JOIN successes s ON f.node_name = s.node_name\n",
    "        GROUP BY f.node_name ORDER BY failure_count DESC\n",
    "    \"\"\"\n",
    "    recovery_df = ddb.execute(recovery_query).pl()\n",
    "    print(recovery_df)\n",
    "    \n",
    "    print(\"\\n\ud83d\udcdd Detailed Error Messages:\")\n",
    "    for row in error_df.iter_rows(named=True):\n",
    "        print(f\"\\n[{row['created_at']}] {row['node_name']}\")\n",
    "        print(f\"  Type: {row['event_type']}\")\n",
    "        print(f\"  Status: {row['status']}\")\n",
    "        if row['result']:\n",
    "            result_str = str(row['result'])\n",
    "            print(f\"  Result: {result_str[:200]}...\" if len(result_str) > 200 else f\"  Result: {result_str}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Run error analysis\n",
    "analyze_errors(EXECUTION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c627bb",
   "metadata": {},
   "source": [
    "## 8. Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d985256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event timeline visualization\n",
    "timeline_query = f\"\"\"\n",
    "    SELECT created_at, event_type, node_name, status\n",
    "    FROM noetl_db.noetl.event\n",
    "    WHERE execution_id = {EXECUTION_ID}\n",
    "    ORDER BY created_at\n",
    "\"\"\"\n",
    "timeline_df = ddb.execute(timeline_query).pl().to_pandas()\n",
    "\n",
    "# Create timeline plot\n",
    "fig = px.scatter(timeline_df, x='created_at', y='event_type', color='status',\n",
    "                 hover_data=['node_name'],\n",
    "                 title=f'Event Timeline - Execution {EXECUTION_ID}',\n",
    "                 labels={'created_at': 'Time', 'event_type': 'Event Type'})\n",
    "fig.update_layout(height=600)\n",
    "fig.show()\n",
    "\n",
    "# Step duration bar chart\n",
    "if len(step_timing_df) > 0:\n",
    "    step_timing_pd = step_timing_df.to_pandas()\n",
    "    fig2 = px.bar(step_timing_pd.nlargest(20, 'duration_seconds'),\n",
    "                  x='duration_seconds', y='node_name', orientation='h',\n",
    "                  title='Top 20 Slowest Steps',\n",
    "                  labels={'duration_seconds': 'Duration (seconds)', 'node_name': 'Step Name'})\n",
    "    fig2.update_layout(height=800, yaxis={'categoryorder': 'total ascending'})\n",
    "    fig2.show()\n",
    "\n",
    "# Event distribution pie chart\n",
    "event_dist = analysis_df.to_pandas()\n",
    "fig3 = px.pie(event_dist, values='event_count', names='event_type',\n",
    "              title='Event Type Distribution')\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d927d",
   "metadata": {},
   "source": [
    "## 9. Historical Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b59b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze recent regression test runs\n",
    "history_query = \"\"\"\n",
    "    WITH test_executions AS (\n",
    "        SELECT DISTINCT e.execution_id,\n",
    "            MIN(e.created_at) as start_time, MAX(e.created_at) as end_time,\n",
    "            EXTRACT(EPOCH FROM (MAX(e.created_at) - MIN(e.created_at))) as duration,\n",
    "            COUNT(DISTINCT CASE WHEN e.event_type = 'step_completed' THEN e.node_name END) as steps_completed,\n",
    "            MAX(CASE WHEN e.event_type = 'playbook_completed' THEN 1 ELSE 0 END) as completed,\n",
    "            MAX(CASE WHEN e.event_type = 'playbook_failed' THEN 1 ELSE 0 END) as failed\n",
    "        FROM noetl_db.noetl.event e\n",
    "        JOIN noetl_db.noetl.catalog c ON e.catalog_id = c.catalog_id\n",
    "        WHERE c.path = 'tests/fixtures/playbooks/regression_test/master_regression_test'\n",
    "            AND e.parent_execution_id IS NULL\n",
    "        GROUP BY e.execution_id\n",
    "    )\n",
    "    SELECT * FROM test_executions\n",
    "    WHERE start_time > NOW() - INTERVAL '7 days'\n",
    "    ORDER BY start_time DESC LIMIT 20\n",
    "\"\"\"\n",
    "history_df = ddb.execute(history_query).pl()\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Recent Test Runs (Last 7 days):\")\n",
    "print(history_df)\n",
    "\n",
    "if len(history_df) > 1:\n",
    "    history_pd = history_df.to_pandas()\n",
    "    \n",
    "    # Success rate over time\n",
    "    fig4 = px.scatter(history_pd, x='start_time', y='steps_completed',\n",
    "                      size='duration', color='completed',\n",
    "                      title='Test Run History',\n",
    "                      labels={'start_time': 'Start Time', 'steps_completed': 'Steps Completed',\n",
    "                              'duration': 'Duration (seconds)', 'completed': 'Completed'})\n",
    "    fig4.add_hline(y=EXPECTED_STEPS, line_dash=\"dash\",\n",
    "                   annotation_text=f\"Expected: {EXPECTED_STEPS} steps\")\n",
    "    fig4.show()\n",
    "    \n",
    "    # Duration trend\n",
    "    fig5 = px.line(history_pd, x='start_time', y='duration',\n",
    "                   title='Test Duration Trend',\n",
    "                   labels={'start_time': 'Start Time', 'duration': 'Duration (seconds)'})\n",
    "    fig5.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\ud83d\udcca Historical Statistics:\")\n",
    "    print(f\"  Total runs: {len(history_df)}\")\n",
    "    print(f\"  Success rate: {(history_df['completed'].sum() / len(history_df) * 100):.1f}%\")\n",
    "    print(f\"  Avg duration: {history_df['duration'].mean():.1f}s\")\n",
    "    print(f\"  Avg steps completed: {history_df['steps_completed'].mean():.1f}/{EXPECTED_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378fd0f",
   "metadata": {},
   "source": [
    "## 10. Export Results & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to Parquet for archival\n",
    "export_dir = \"/home/jovyan/work/test_results\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Export event data\n",
    "events_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM noetl_db.noetl.event\n",
    "    WHERE execution_id = {EXECUTION_ID}\n",
    "\"\"\"\n",
    "events_arrow = ddb.execute(events_query).arrow()\n",
    "pq.write_table(events_arrow, f\"{export_dir}/test_{EXECUTION_ID}_events.parquet\")\n",
    "\n",
    "# Export validation results as JSON\n",
    "with open(f\"{export_dir}/test_{EXECUTION_ID}_validation.json\", 'w') as f:\n",
    "    json.dump(validation_result, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\u2713 Results exported to {export_dir}\")\n",
    "print(f\"  - test_{EXECUTION_ID}_events.parquet\")\n",
    "print(f\"  - test_{EXECUTION_ID}_validation.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de6e4e",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dcd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close DuckDB connection\n",
    "ddb.close()\n",
    "print(\"\u2713 Connections closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eee1eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive regression testing with:\n",
    "- \u2705 Modern data stack (psycopg3, DuckDB, Polars, Arrow)\n",
    "- \u2705 Real-time execution monitoring\n",
    "- \u2705 Comprehensive validation\n",
    "- \u2705 Error detection and recovery analysis\n",
    "- \u2705 Performance visualizations\n",
    "- \u2705 Historical trend analysis\n",
    "- \u2705 Result archival\n",
    "\n",
    "**Next Steps:**\n",
    "1. Deploy JupyterLab to Kubernetes cluster\n",
    "2. Schedule regular regression test runs\n",
    "3. Set up alerting for test failures\n",
    "4. Integrate with CI/CD pipeline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}