apiVersion: noetl.io/v2
kind: Playbook
metadata:
  name: gke_autopilot_cluster
  path: iap/gcp/gke-autopilot
  description: Provision and manage GKE Autopilot clusters
  labels:
    iap.noetl.io/provider: gcp
    iap.noetl.io/resource-type: container.googleapis.com/Cluster
    iap.noetl.io/mode: terraform

executor:
  profile: local
  version: noetl-runtime/1
workload:
  # Action: create, deploy, update, destroy, plan
  action: create

  # Required parameters - must be provided via --set project_id=<your-project>
  project_id: ""  # e.g., noetl-demo-19700101
  cluster_name: noetl-cluster
  region: us-central1

  # Network configuration
  network: default
  subnetwork: default
  master_ipv4_cidr: 172.16.0.0/28

  # Cluster configuration
  release_channel: REGULAR  # RAPID, REGULAR, STABLE

  # Private cluster settings
  enable_private_nodes: true
  enable_private_endpoint: false

  # State management
  state_database: .noetl/state.duckdb
  state_bucket: ""  # Defaults to {project_id}-noetl-state
  workspace: default

  # Optional: deploy NoETL stack after cluster is ready
  deploy_stack: false

  # Helm deployment configuration (required when deploy_stack=true or action=deploy)
  postgres_namespace: postgres
  postgres_release_name: postgres
  postgres_storage_class: standard
  postgres_size: 20Gi
  postgres_database: noetl
  postgres_user: noetl
  postgres_password: demo
  postgres_image_registry: ""  # e.g., us-central1-docker.pkg.dev
  postgres_image_repository: ""  # e.g., {project_id}/noetl/bitnami-postgresql
  postgres_image_tag: ""  # e.g., multiarch or 16
  postgres_use_bitnami: true  # Use bitnami/postgresql from public Helm repo
  postgres_allow_insecure_images: true
  init_noetl_schema: true
  noetl_schema_path: noetl/database/ddl/postgres/schema_ddl.sql

  nats_namespace: nats
  nats_user: noetl
  nats_password: noetl
  nats_jetstream_file_size: 5Gi
  nats_jetstream_mem_size: 1Gi

  # ClickHouse configuration
  clickhouse_enabled: true
  clickhouse_namespace: clickhouse
  clickhouse_storage_size: 10Gi
  clickhouse_storage_class: standard
  clickhouse_memory_limit: 2Gi
  clickhouse_cpu_limit: 2000m

  noetl_namespace: noetl
  noetl_image_repository: ""  # e.g., us-central1-docker.pkg.dev/{project_id}/noetl/noetl
  noetl_image_tag: latest
  noetl_image_pull_policy: IfNotPresent
  noetl_external_service_enabled: false
  noetl_external_service_type: ClusterIP
  noetl_service_port: 8082
  noetl_data_storage_class: standard-rwx
  noetl_data_size: 10Gi
  noetl_logs_storage_class: standard
  noetl_logs_size: 5Gi
  noetl_password: noetl
  noetl_persistence_data_enabled: false
  noetl_persistence_logs_enabled: false
  noetl_worker_pool_enabled: false
  noetl_ingress_enabled: true
  noetl_ingress_class: gce
  noetl_ingress_host: ""  # e.g., api.{domain}
  noetl_ingress_managed_cert_enabled: true
  noetl_ingress_managed_cert_name: noetl-managed-cert
  noetl_ingress_allow_http: false

  gateway_namespace: gateway
  gateway_image_repository: ""  # e.g., us-central1-docker.pkg.dev/{project_id}/noetl/noetl-gateway
  gateway_image_tag: latest
  gateway_image_pull_policy: IfNotPresent
  gateway_service_type: ClusterIP
  gateway_service_port: 8090
  gateway_noetl_base_url: http://noetl.noetl.svc.cluster.local:8082
  gateway_ingress_enabled: true
  gateway_ingress_class: gce
  gateway_ingress_host: ""  # e.g., gateway.{domain}
  gateway_ingress_managed_cert_enabled: true
  gateway_ingress_managed_cert_name: gateway-managed-cert
  gateway_ingress_allow_http: false

workflow:
  - step: start
    desc: Initialize GKE Autopilot workflow
    tool:
      kind: shell
      cmds:
        - |
          echo "GKE Autopilot Cluster Management"
          echo "================================"
          echo "Action:   {{ workload.action }}"
          echo "Project:  {{ workload.project_id }}"
          echo "Cluster:  {{ workload.cluster_name }}"
          echo "Region:   {{ workload.region }}"
          if [ -z "{{ workload.project_id }}" ]; then
            echo ""
            echo "ERROR: project_id is required"
            echo "Usage: noetl run automation/iap/gcp/gke_autopilot.yaml --set project_id=<your-gcp-project> --set action=<action>"
            exit 1
          fi
    vars:
      action: "{{ workload.action }}"
      deploy_stack: "{{ workload.deploy_stack | default(false) }}"
      resource_id: "{{ workload.project_id }}/{{ workload.region }}/{{ workload.cluster_name }}"
      state_bucket: "{{ workload.state_bucket if workload.state_bucket else workload.project_id + '-noetl-state' }}"
    case:
      - when: "{{ workload.action }} == create"
        then:
          - step: enable_apis
      - when: "{{ workload.action }} == deploy"
        then:
          - step: check_existing
      - when: "{{ workload.action }} == destroy"
        then:
          - step: delete_cluster
      - when: "{{ workload.action }} == plan"
        then:
          - step: plan_changes
      - when: "true"
        then:
          - step: show_help

  - step: show_help
    desc: Show available actions
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "Usage: noetl run automation/iap/gcp/gke_autopilot.yaml --set action=<action> --set project_id=<project>"
          echo ""
          echo "Available actions:"
          echo "  create  - Create a new GKE Autopilot cluster"
          echo "  deploy  - Deploy NoETL stack to an existing cluster"
          echo "  destroy - Delete an existing cluster"
          echo "  plan    - Show planned changes without applying"
          echo ""
          echo "Required parameters:"
          echo "  project_id     - GCP project ID (e.g., noetl-demo-19700101)"
          echo ""
          echo "Optional parameters:"
          echo "  cluster_name   - Cluster name (default: noetl-cluster)"
          echo "  region         - GCP region (default: us-central1)"
          echo ""
          echo "Example - Create cluster:"
          echo "  noetl run automation/iap/gcp/gke_autopilot.yaml \\"
          echo "    --set action=create \\"
          echo "    --set project_id=noetl-demo-19700101"
          echo ""
          echo "Example - Deploy stack to existing cluster:"
          echo "  noetl run automation/iap/gcp/gke_autopilot.yaml \\"
          echo "    --set action=deploy \\"
          echo "    --set project_id=noetl-demo-19700101 \\"
          echo "    --set noetl_image_repository=us-central1-docker.pkg.dev/noetl-demo-19700101/noetl/noetl \\"
          echo "    --set noetl_image_tag=latest \\"
          echo "    --set gateway_image_repository=us-central1-docker.pkg.dev/noetl-demo-19700101/noetl/noetl-gateway \\"
          echo "    --set gateway_image_tag=latest"
          echo ""
          echo "Example - Create and deploy in one step:"
          echo "  noetl run automation/iap/gcp/gke_autopilot.yaml \\"
          echo "    --set action=create \\"
          echo "    --set deploy_stack=true \\"
          echo "    --set project_id=noetl-demo-19700101 \\"
          echo "    --set noetl_image_repository=us-central1-docker.pkg.dev/noetl-demo-19700101/noetl/noetl \\"
          echo "    --set noetl_image_tag=latest"
          echo ""
          echo "Components deployed (when deploy_stack=true):"
          echo "  - PostgreSQL (Bitnami Helm chart)"
          echo "  - NATS JetStream"
          echo "  - ClickHouse (when clickhouse_enabled=true)"
          echo "  - NoETL Server and Workers"
          echo "  - NoETL Gateway"
    next:
      - step: end

  - step: check_existing
    desc: Check if cluster already exists
    tool:
      kind: http
      method: GET
      url: https://container.googleapis.com/v1/projects/{{ workload.project_id }}/locations/{{ workload.region }}/clusters/{{ workload.cluster_name }}
      auth:
        source: adc
    vars:
      cluster_status: "{{ result.status }}"
    case:
      - when: "{{ vars.cluster_status }} == 200"
        then:
          - step: get_cluster_details
      - when: "true"
        then:
          - step: deploy_cluster_not_found
      - when: "true"
        then:
          - step: enable_apis

  - step: cluster_exists_info
    desc: Cluster already exists
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "Cluster '{{ workload.cluster_name }}' already exists in project '{{ workload.project_id }}'"
          echo "Status: {{ vars.current_state.status | default('unknown') }}"
          echo ""
          echo "To update the cluster, use action=update (not yet implemented)"
          echo "To delete the cluster, use action=destroy"
    next:
      - step: update_state

  - step: enable_apis
    desc: Enable required GCP APIs
    tool:
      kind: shell
      cmds:
        - |
          echo "Enabling Container API..."
          gcloud services enable container.googleapis.com --project {{ workload.project_id }}
          echo "Container API enabled."
    next:
      - step: create_cluster

  - step: create_cluster
    desc: Create GKE Autopilot cluster
    tool:
      kind: http
      method: POST
      url: https://container.googleapis.com/v1/projects/{{ workload.project_id }}/locations/{{ workload.region }}/clusters
      auth:
        source: adc
      headers:
        Content-Type: application/json
      body: |
        {
          "cluster": {
            "name": "{{ workload.cluster_name }}",
            "description": "GKE Autopilot cluster managed by NoETL IaP",
            "autopilot": {
              "enabled": true
            },
            "network": "projects/{{ workload.project_id }}/global/networks/{{ workload.network }}",
            "subnetwork": "projects/{{ workload.project_id }}/regions/{{ workload.region }}/subnetworks/{{ workload.subnetwork }}",
            "releaseChannel": {
              "channel": "{{ workload.release_channel }}"
            },
            "privateClusterConfig": {
              "enablePrivateNodes": {{ workload.enable_private_nodes | lower }},
              "enablePrivateEndpoint": {{ workload.enable_private_endpoint | lower }},
              "masterIpv4CidrBlock": "{{ workload.master_ipv4_cidr }}"
            },
            "ipAllocationPolicy": {
              "useIpAliases": true
            },
            "resourceLabels": {
              "managed-by": "noetl-iap",
              "workspace": "{{ workload.workspace }}"
            }
          }
        }
    vars:
      operation_name: "{{ result.body.name | default('') }}"
      create_status: "{{ result.status }}"
    case:
      - when: "{{ vars.create_status }} == 200"
        then:
          - step: wait_for_operation
      - when: "true"
        then:
          - step: create_error

  - step: create_error
    desc: Handle cluster creation error
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "ERROR: Failed to create cluster"
          echo "Status: {{ vars.create_status }}"
          echo ""
          echo "Common issues:"
          echo "  - Insufficient permissions (need roles/container.admin)"
          echo "  - Container API not enabled"
          echo "  - Network/subnet doesn't exist"
          echo "  - CIDR range conflicts"
          exit 1
    next:
      - step: end

  - step: deploy_cluster_not_found
    desc: Cluster not found for deploy action
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "ERROR: Cluster '{{ workload.cluster_name }}' not found in project '{{ workload.project_id }}'"
          echo "Action 'deploy' requires an existing cluster. Use action=create first."
          exit 1
    next:
      - step: end

  - step: wait_for_operation
    desc: Wait for cluster creation to complete
    tool:
      kind: rhai
      code: |
        // Get GCP auth token
        let token = get_gcp_token();
        let url = `https://container.googleapis.com/v1/projects/${args.project_id}/locations/${args.region}/clusters/${args.cluster_name}`;
        
        log("");
        log(`Cluster creation initiated. Operation: ${args.operation_name}`);
        log("Waiting for cluster to be ready (this may take 5-10 minutes)...");
        log("");
        
        let max_retries = 60;
        let retry_interval = 10;
        let retry_count = 0;
        let final_status = "UNKNOWN";
        
        while retry_count < max_retries {
          retry_count += 1;
          
          let response = http_get_auth(url, token);
          
          if response.status == 200 {
            let cluster_status = response.body.status;
            log(`[${timestamp()}] Cluster status: ${cluster_status} (attempt ${retry_count}/${max_retries})`);
            
            if cluster_status == "RUNNING" {
              log("");
              log("Cluster is now RUNNING");
              final_status = "RUNNING";
              break;
            } else if cluster_status == "ERROR" || cluster_status == "DEGRADED" {
              log("");
              log(`ERROR: Cluster reached error state: ${cluster_status}`);
              final_status = cluster_status;
              throw `Cluster creation failed with status: ${cluster_status}`;
            }
            // Still provisioning
          } else if response.status == 404 {
            log(`[${timestamp()}] Cluster not yet visible (attempt ${retry_count}/${max_retries})`);
          } else {
            log(`[${timestamp()}] Unexpected response: ${response.status}`);
          }
          
          sleep(retry_interval);
        }
        
        if final_status != "RUNNING" {
          throw `Timeout waiting for cluster after ${max_retries * retry_interval} seconds`;
        }
        
        #{ status: final_status, attempts: retry_count }
      args:
        project_id: "{{ workload.project_id }}"
        cluster_name: "{{ workload.cluster_name }}"
        region: "{{ workload.region }}"
        operation_name: "{{ vars.operation_name }}"
    next:
      - step: get_cluster_details

  - step: get_cluster_details
    desc: Get created cluster details
    tool:
      kind: http
      method: GET
      url: https://container.googleapis.com/v1/projects/{{ workload.project_id }}/locations/{{ workload.region }}/clusters/{{ workload.cluster_name }}
      auth:
        source: adc
    vars:
      current_state: "{{ result.body }}"
      cluster_endpoint: "{{ result.body.endpoint | default('') }}"
      cluster_status: "{{ result.body.status | default('UNKNOWN') }}"
    next:
      - step: maybe_deploy_stack

  - step: maybe_deploy_stack
    desc: Deploy NoETL stack if requested
    tool:
      kind: shell
      cmds:
        - |
          echo "Deploy stack flag: {{ workload.deploy_stack }}"
    case:
      - when: "{{ workload.deploy_stack }} == true or {{ workload.action }} == deploy"
        then:
          - step: configure_kubecontext
      - when: "true"
        then:
          - step: save_state

  - step: configure_kubecontext
    desc: Configure kubectl context for the GKE cluster
    tool:
      kind: shell
      cmds:
        - |
          echo "Configuring kubectl context..."
          gcloud container clusters get-credentials {{ workload.cluster_name }} \
            --region {{ workload.region }} \
            --project {{ workload.project_id }}
          kubectl config current-context
    next:
      - step: validate_deploy_inputs

  - step: validate_deploy_inputs
    desc: Validate Helm deployment inputs
    tool:
      kind: shell
      cmds:
        - |
          if [ -z "{{ workload.noetl_image_repository }}" ]; then
            echo "ERROR: noetl_image_repository is required for deployment"
            exit 1
          fi
          if [ -z "{{ workload.gateway_image_repository }}" ]; then
            echo "ERROR: gateway_image_repository is required for deployment"
            exit 1
          fi
    next:
      - step: add_helm_repos

  - step: add_helm_repos
    desc: Add Helm repositories
    tool:
      kind: shell
      cmds:
        - |
          echo "Adding Helm repositories..."
          helm repo add nats https://nats-io.github.io/k8s/helm/charts/
          helm repo add bitnami https://charts.bitnami.com/bitnami
          helm repo update
    next:
      - step: deploy_postgres_helm

  - step: deploy_postgres_helm
    desc: Deploy PostgreSQL via Helm
    tool:
      kind: shell
      cmds:
        - |
          echo "Deploying PostgreSQL..."
          helm upgrade --install {{ workload.postgres_release_name }} bitnami/postgresql \
            --namespace {{ workload.postgres_namespace }} \
            --create-namespace \
            --set fullnameOverride=postgres \
            --set auth.postgresPassword={{ workload.postgres_password }} \
            --set auth.username={{ workload.postgres_user }} \
            --set auth.password={{ workload.noetl_password }} \
            --set auth.database={{ workload.postgres_database }} \
            --set primary.persistence.size={{ workload.postgres_size }} \
            --set primary.persistence.storageClass={{ workload.postgres_storage_class }} \
            --set architecture=standalone \
            --set image.registry={{ workload.postgres_image_registry }} \
            --set image.repository={{ workload.postgres_image_repository }} \
            --set image.tag={{ workload.postgres_image_tag }} \
            --set global.security.allowInsecureImages={{ workload.postgres_allow_insecure_images | lower }}
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance={{ workload.postgres_release_name }} -n {{ workload.postgres_namespace }} --timeout=180s
    next:
      - step: maybe_init_noetl_schema

  - step: maybe_init_noetl_schema
    desc: Initialize NoETL schema if enabled
    tool:
      kind: shell
      cmds:
        - |
          echo "Init NoETL schema flag: {{ workload.init_noetl_schema }}"
    case:
      - when: "{{ workload.init_noetl_schema }} == true"
        then:
          - step: init_noetl_schema
      - when: "true"
        then:
          - step: deploy_nats_helm

  - step: init_noetl_schema
    desc: Initialize NoETL schema in PostgreSQL
    tool:
      kind: shell
      cmds:
        - |
          echo "Initializing NoETL schema..."
          POSTGRES_POD=$(kubectl get pods -n {{ workload.postgres_namespace }} -l app.kubernetes.io/instance={{ workload.postgres_release_name }} -o jsonpath='{.items[0].metadata.name}')
          if [ -z "$POSTGRES_POD" ]; then
            echo "ERROR: PostgreSQL pod not found"
            exit 1
          fi
          if [ ! -f "{{ workload.noetl_schema_path }}" ]; then
            echo "ERROR: NoETL schema file not found: {{ workload.noetl_schema_path }}"
            echo "Set --var noetl_schema_path=/absolute/path/to/schema_ddl.sql"
            exit 1
          fi
          TABLE_EXISTS=$(kubectl exec -n {{ workload.postgres_namespace }} "$POSTGRES_POD" -- /bin/sh -c "PGPASSWORD={{ workload.postgres_password }} psql -U postgres -d {{ workload.postgres_database }} -tAc \"SELECT 1 FROM information_schema.tables WHERE table_schema='noetl' AND table_name='event'\"")
          if [ "$TABLE_EXISTS" = "1" ]; then
            echo "NoETL schema already present. Skipping DDL."
            exit 0
          fi
          kubectl exec -n {{ workload.postgres_namespace }} "$POSTGRES_POD" -- /bin/sh -c "PGPASSWORD={{ workload.postgres_password }} psql -U postgres -d {{ workload.postgres_database }} -c \"CREATE SCHEMA IF NOT EXISTS noetl\""
          kubectl exec -i -n {{ workload.postgres_namespace }} "$POSTGRES_POD" -- /bin/sh -c "PGPASSWORD={{ workload.postgres_password }} psql -U postgres -d {{ workload.postgres_database }}" < {{ workload.noetl_schema_path }}
    next:
      - step: deploy_nats_helm

  - step: deploy_nats_helm
    desc: Deploy NATS JetStream via Helm
    tool:
      kind: shell
      cmds:
        - |
          echo "Deploying NATS JetStream..."
          helm upgrade --install nats nats/nats \
            --namespace {{ workload.nats_namespace }} \
            --create-namespace \
            --set config.jetstream.enabled=true \
            --set config.jetstream.fileStore.enabled=true \
            --set config.jetstream.fileStore.pvc.size={{ workload.nats_jetstream_file_size }} \
            --set config.jetstream.memoryStore.enabled=true \
            --set config.jetstream.memoryStore.maxSize={{ workload.nats_jetstream_mem_size }} \
            --set 'config.merge.authorization.users[0].user={{ workload.nats_user }}' \
            --set 'config.merge.authorization.users[0].password={{ workload.nats_password }}'
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=nats -n {{ workload.nats_namespace }} --timeout=180s
    next:
      - step: maybe_deploy_clickhouse

  - step: maybe_deploy_clickhouse
    desc: Deploy ClickHouse if enabled
    tool:
      kind: shell
      cmds:
        - |
          echo "ClickHouse enabled: {{ workload.clickhouse_enabled }}"
    case:
      - when: "{{ workload.clickhouse_enabled }} == true"
        then:
          - step: deploy_clickhouse
      - when: "true"
        then:
          - step: deploy_noetl_helm

  - step: deploy_clickhouse
    desc: Deploy ClickHouse for observability
    tool:
      kind: shell
      cmds:
        - |
          echo "Deploying ClickHouse..."
          kubectl create namespace {{ workload.clickhouse_namespace }} --dry-run=client -o yaml | kubectl apply -f -

          # Create ClickHouse ConfigMap
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: clickhouse-config
            namespace: {{ workload.clickhouse_namespace }}
          data:
            users.xml: |
              <clickhouse>
                <users>
                  <default>
                    <password></password>
                    <networks>
                      <ip>::/0</ip>
                    </networks>
                    <profile>default</profile>
                    <quota>default</quota>
                  </default>
                </users>
              </clickhouse>
            config.xml: |
              <clickhouse>
                <logger>
                  <level>information</level>
                  <console>true</console>
                </logger>
                <http_port>8123</http_port>
                <tcp_port>9000</tcp_port>
                <listen_host>::</listen_host>
                <max_connections>4096</max_connections>
                <path>/var/lib/clickhouse/</path>
                <tmp_path>/var/lib/clickhouse/tmp/</tmp_path>
              </clickhouse>
          EOF

          # Create ClickHouse Service
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Service
          metadata:
            name: clickhouse
            namespace: {{ workload.clickhouse_namespace }}
            labels:
              app: clickhouse
          spec:
            type: ClusterIP
            ports:
              - port: 8123
                targetPort: 8123
                protocol: TCP
                name: http
              - port: 9000
                targetPort: 9000
                protocol: TCP
                name: native
            selector:
              app: clickhouse
          EOF

          # Create ClickHouse StatefulSet
          cat <<EOF | kubectl apply -f -
          apiVersion: apps/v1
          kind: StatefulSet
          metadata:
            name: clickhouse
            namespace: {{ workload.clickhouse_namespace }}
            labels:
              app: clickhouse
          spec:
            serviceName: clickhouse
            replicas: 1
            selector:
              matchLabels:
                app: clickhouse
            template:
              metadata:
                labels:
                  app: clickhouse
              spec:
                containers:
                - name: clickhouse
                  image: clickhouse/clickhouse-server:24.11
                  ports:
                  - name: http
                    containerPort: 8123
                  - name: native
                    containerPort: 9000
                  volumeMounts:
                  - name: data
                    mountPath: /var/lib/clickhouse
                  - name: config
                    mountPath: /etc/clickhouse-server/config.d/
                  - name: users
                    mountPath: /etc/clickhouse-server/users.d/
                  resources:
                    requests:
                      memory: "512Mi"
                      cpu: "500m"
                    limits:
                      memory: "{{ workload.clickhouse_memory_limit }}"
                      cpu: "{{ workload.clickhouse_cpu_limit }}"
                  livenessProbe:
                    httpGet:
                      path: /ping
                      port: 8123
                    initialDelaySeconds: 30
                    periodSeconds: 10
                  readinessProbe:
                    httpGet:
                      path: /ping
                      port: 8123
                    initialDelaySeconds: 10
                    periodSeconds: 5
                volumes:
                - name: config
                  configMap:
                    name: clickhouse-config
                    items:
                    - key: config.xml
                      path: config.xml
                - name: users
                  configMap:
                    name: clickhouse-config
                    items:
                    - key: users.xml
                      path: users.xml
            volumeClaimTemplates:
            - metadata:
                name: data
              spec:
                accessModes: [ "ReadWriteOnce" ]
                storageClassName: {{ workload.clickhouse_storage_class }}
                resources:
                  requests:
                    storage: {{ workload.clickhouse_storage_size }}
          EOF

          echo "Waiting for ClickHouse to be ready..."
          kubectl wait --for=condition=ready pod -l app=clickhouse -n {{ workload.clickhouse_namespace }} --timeout=180s || true
          echo "ClickHouse deployed."
    next:
      - step: deploy_noetl_helm

  - step: deploy_noetl_helm
    desc: Deploy NoETL server and workers via Helm
    tool:
      kind: shell
      cmds:
        - |
          echo "Deploying NoETL..."
          helm upgrade --install noetl ./automation/helm/noetl \
            --namespace {{ workload.noetl_namespace }} \
            --create-namespace \
            --set image.repository={{ workload.noetl_image_repository }} \
            --set image.tag={{ workload.noetl_image_tag }} \
            --set image.pullPolicy={{ workload.noetl_image_pull_policy }} \
            --set externalService.enabled={{ workload.noetl_external_service_enabled | lower }} \
            --set externalService.type={{ workload.noetl_external_service_type }} \
            --set externalService.port={{ workload.noetl_service_port }} \
            --set persistence.data.enabled={{ workload.noetl_persistence_data_enabled | lower }} \
            --set persistence.logs.enabled={{ workload.noetl_persistence_logs_enabled | lower }} \
            --set persistence.data.storageClassName={{ workload.noetl_data_storage_class }} \
            --set persistence.data.size={{ workload.noetl_data_size }} \
            --set persistence.logs.storageClassName={{ workload.noetl_logs_storage_class }} \
            --set persistence.logs.size={{ workload.noetl_logs_size }} \
            --set workerPool.enabled={{ workload.noetl_worker_pool_enabled | lower }} \
            --set ingress.enabled={{ workload.noetl_ingress_enabled | lower }} \
            --set ingress.className={{ workload.noetl_ingress_class }} \
            --set ingress.host={{ workload.noetl_ingress_host }} \
            --set ingress.managedCertificate.enabled={{ workload.noetl_ingress_managed_cert_enabled | lower }} \
            --set ingress.managedCertificate.name={{ workload.noetl_ingress_managed_cert_name }} \
            --set-string ingress.annotations.kubernetes\.io/ingress\.allow-http={{ workload.noetl_ingress_allow_http | lower }} \
            --set-string secrets.postgresPassword={{ workload.postgres_password }} \
            --set-string secrets.noetlPassword={{ workload.noetl_password }}
          kubectl wait --for=condition=ready pod -l app=noetl-server -n {{ workload.noetl_namespace }} --timeout=180s || true
          kubectl wait --for=condition=ready pod -l app=noetl-worker -n {{ workload.noetl_namespace }} --timeout=180s || true
    next:
      - step: deploy_gateway_helm

  - step: deploy_gateway_helm
    desc: Deploy NoETL Gateway via Helm
    tool:
      kind: shell
      cmds:
        - |
          echo "Deploying NoETL Gateway..."
          helm upgrade --install noetl-gateway ./automation/helm/gateway \
            --namespace {{ workload.gateway_namespace }} \
            --create-namespace \
            --set image.repository={{ workload.gateway_image_repository }} \
            --set image.tag={{ workload.gateway_image_tag }} \
            --set image.pullPolicy={{ workload.gateway_image_pull_policy }} \
            --set service.type={{ workload.gateway_service_type }} \
            --set service.port={{ workload.gateway_service_port }} \
            --set-string env.noetlBaseUrl={{ workload.gateway_noetl_base_url }} \
            --set ingress.enabled={{ workload.gateway_ingress_enabled | lower }} \
            --set ingress.className={{ workload.gateway_ingress_class }} \
            --set ingress.host={{ workload.gateway_ingress_host }} \
            --set ingress.managedCertificate.enabled={{ workload.gateway_ingress_managed_cert_enabled | lower }} \
            --set ingress.managedCertificate.name={{ workload.gateway_ingress_managed_cert_name }} \
            --set-string ingress.annotations.kubernetes\.io/ingress\.allow-http={{ workload.gateway_ingress_allow_http | lower }}
          kubectl wait --for=condition=ready pod -l app=gateway -n {{ workload.gateway_namespace }} --timeout=180s || true
    next:
      - step: save_state

  - step: save_state
    desc: Save cluster state to DuckDB
    tool:
      kind: duckdb
      database: "{{ workload.state_database }}"
      commands: |
        INSERT INTO resources (
          resource_id, resource_type, resource_name, provider, project, region,
          zone, status, config, state
        )
        VALUES (
          '{{ vars.resource_id }}',
          'container.googleapis.com/Cluster',
          '{{ workload.cluster_name }}',
          'gcp',
          '{{ workload.project_id }}',
          '{{ workload.region }}',
          NULL,
          '{{ vars.cluster_status | default("pending") }}',
          json('{
            "cluster_name": "{{ workload.cluster_name }}",
            "region": "{{ workload.region }}",
            "network": "{{ workload.network }}",
            "release_channel": "{{ workload.release_channel }}",
            "enable_private_nodes": {{ workload.enable_private_nodes | lower }}
          }'),
          json('{{ vars.current_state | tojson | default("{}") }}')
        )
        ON CONFLICT (resource_id) DO UPDATE SET
          resource_type = EXCLUDED.resource_type,
          resource_name = EXCLUDED.resource_name,
          provider = EXCLUDED.provider,
          project = EXCLUDED.project,
          region = EXCLUDED.region,
          zone = EXCLUDED.zone,
          status = EXCLUDED.status,
          config = EXCLUDED.config,
          state = EXCLUDED.state,
          updated_at = CURRENT_TIMESTAMP,
          last_sync_at = CURRENT_TIMESTAMP;
    next:
      - step: print_success

  - step: update_state
    desc: Update state for existing cluster
    tool:
      kind: duckdb
      database: "{{ workload.state_database }}"
      commands: |
        INSERT INTO resources (
          resource_id, resource_type, resource_name, provider, project, region,
          zone, status, config, state
        )
        VALUES (
          '{{ vars.resource_id }}',
          'container.googleapis.com/Cluster',
          '{{ workload.cluster_name }}',
          'gcp',
          '{{ workload.project_id }}',
          '{{ workload.region }}',
          NULL,
          '{{ vars.current_state.status | default("unknown") }}',
          json('{}'),
          json('{{ vars.current_state | tojson | default("{}") }}')
        )
        ON CONFLICT (resource_id) DO UPDATE SET
          resource_type = EXCLUDED.resource_type,
          resource_name = EXCLUDED.resource_name,
          provider = EXCLUDED.provider,
          project = EXCLUDED.project,
          region = EXCLUDED.region,
          zone = EXCLUDED.zone,
          status = EXCLUDED.status,
          config = EXCLUDED.config,
          state = EXCLUDED.state,
          updated_at = CURRENT_TIMESTAMP,
          last_sync_at = CURRENT_TIMESTAMP;
    next:
      - step: end

  - step: print_success
    desc: Print success message
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "=========================================="
          echo " GKE Autopilot Cluster Created"
          echo "=========================================="
          echo ""
          echo "Cluster:   {{ workload.cluster_name }}"
          echo "Project:   {{ workload.project_id }}"
          echo "Region:    {{ workload.region }}"
          echo "Status:    {{ vars.cluster_status }}"
          echo "Endpoint:  {{ vars.cluster_endpoint }}"
          echo ""
          echo "To connect to the cluster:"
          echo "  gcloud container clusters get-credentials {{ workload.cluster_name }} \\"
          echo "    --region {{ workload.region }} \\"
          echo "    --project {{ workload.project_id }}"
          echo ""
          echo "State saved to: {{ workload.state_database }}"
          echo ""
    next:
      - step: end

  - step: delete_cluster
    desc: Delete GKE Autopilot cluster
    tool:
      kind: http
      method: DELETE
      url: https://container.googleapis.com/v1/projects/{{ workload.project_id }}/locations/{{ workload.region }}/clusters/{{ workload.cluster_name }}
      auth:
        source: adc
    vars:
      delete_status: "{{ result.status }}"
      delete_operation: "{{ result.body.name | default('') }}"
    case:
      - when: "{{ vars.delete_status }} == 200"
        then:
          - step: wait_delete
      - when: "{{ vars.delete_status }} == 404"
        then:
          - step: cluster_not_found
      - when: "true"
        then:
          - step: delete_error

  - step: cluster_not_found
    desc: Cluster doesn't exist
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "Cluster '{{ workload.cluster_name }}' not found in project '{{ workload.project_id }}'"
          echo "Nothing to delete."
    next:
      - step: remove_from_state

  - step: delete_error
    desc: Handle delete error
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "ERROR: Failed to delete cluster"
          echo "Status: {{ vars.delete_status }}"
          exit 1
    next:
      - step: end

  - step: wait_delete
    desc: Wait for cluster deletion to complete
    tool:
      kind: rhai
      code: |
        // Get GCP auth token
        let token = get_gcp_token();
        let url = `https://container.googleapis.com/v1/projects/${args.project_id}/locations/${args.region}/clusters/${args.cluster_name}`;
        
        log("");
        log(`Cluster deletion initiated. Operation: ${args.delete_operation}`);
        log("Waiting for deletion to complete...");
        log("");
        
        let max_retries = 60;
        let retry_interval = 10;
        let retry_count = 0;
        
        while retry_count < max_retries {
          retry_count += 1;
          
          let response = http_get_auth(url, token);
          
          if response.status == 404 {
            // Cluster is gone
            log("");
            log(`[${timestamp()}] Cluster deleted successfully`);
            return #{ status: "DELETED", attempts: retry_count };
          } else if response.status == 200 {
            let cluster_status = response.body.status;
            log(`[${timestamp()}] Cluster status: ${cluster_status} (attempt ${retry_count}/${max_retries})`);
            
            if cluster_status == "ERROR" {
              throw "Cluster deletion failed";
            }
          } else {
            // Check if error contains "not found"
            if contains_any(response.body_raw, ["not found", "NOT_FOUND", "404"]) {
              log("");
              log(`[${timestamp()}] Cluster deleted successfully`);
              return #{ status: "DELETED", attempts: retry_count };
            }
            log(`[${timestamp()}] Unexpected response: ${response.status}`);
          }
          
          sleep(retry_interval);
        }
        
        throw `Timeout waiting for cluster deletion after ${max_retries * retry_interval} seconds`;
      args:
        project_id: "{{ workload.project_id }}"
        cluster_name: "{{ workload.cluster_name }}"
        region: "{{ workload.region }}"
        delete_operation: "{{ vars.delete_operation }}"
    next:
      - step: remove_from_state

  - step: remove_from_state
    desc: Remove cluster from state
    tool:
      kind: duckdb
      database: "{{ workload.state_database }}"
      commands: |
        -- Log the delete operation
        INSERT INTO operations (
          operation_id, resource_id, operation_type, status,
          playbook_name
        )
        SELECT 
          '{{ workload.project_id }}-{{ workload.cluster_name }}-delete-' || strftime(CURRENT_TIMESTAMP, '%Y%m%d%H%M%S'),
          '{{ vars.resource_id }}',
          'delete',
          'completed',
          'gke_autopilot'
        WHERE EXISTS (SELECT 1 FROM resources WHERE resource_id = '{{ vars.resource_id }}');

        -- Delete the resource
        DELETE FROM resources WHERE resource_id = '{{ vars.resource_id }}';

        SELECT 'Resource removed from state' as message;
    next:
      - step: print_delete_success

  - step: print_delete_success
    desc: Print delete success message
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "=========================================="
          echo " GKE Autopilot Cluster Deleted"
          echo "=========================================="
          echo ""
          echo "Cluster:   {{ workload.cluster_name }}"
          echo "Project:   {{ workload.project_id }}"
          echo "Region:    {{ workload.region }}"
          echo ""
          echo "Resource removed from state database."
          echo ""
    next:
      - step: end

  - step: plan_changes
    desc: Plan changes without applying
    tool:
      kind: shell
      cmds:
        - |
          echo ""
          echo "Plan mode not yet fully implemented."
          echo ""
          echo "Would create GKE Autopilot cluster:"
          echo "  - Name:           {{ workload.cluster_name }}"
          echo "  - Project:        {{ workload.project_id }}"
          echo "  - Region:         {{ workload.region }}"
          echo "  - Network:        {{ workload.network }}"
          echo "  - Release:        {{ workload.release_channel }}"
          echo "  - Private Nodes:  {{ workload.enable_private_nodes }}"
          echo ""
    next:
      - step: end

  - step: end
    desc: Workflow complete
